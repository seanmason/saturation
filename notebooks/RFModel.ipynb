{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0862e1de-50d4-424b-86e2-25fd84456ee8",
   "metadata": {},
   "source": [
    "#### Data preparation\n",
    "- 243 simulations, 3 per point in the CCD design\n",
    "- Hold out 1 simulation per CCD point for testing\n",
    "- From the final 50,000 steps of each simulation, take a random sample of 5,000\n",
    "\n",
    "#### Models\n",
    "$\\gamma = \\beta * X + \\epsilon$\n",
    "\n",
    "$Z = \\beta * X + \\epsilon$\n",
    "\n",
    "$Za = \\beta * X + \\epsilon$\n",
    "\n",
    "where $\\gamma$ is areal density and X is the design matrix.\n",
    "\n",
    "Predictor variables (PVs) in the design matrix are:\n",
    "- slope\n",
    "- R_stat multiplier\n",
    "- Min rim percentage\n",
    "- Effective radius multiplier\n",
    "- Number of craters in the study region"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0a8697b0-37cd-4be9-9d9e-77a08a58a36a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import plotly.express as px\n",
    "import plotly.io as pio\n",
    "import plotly.graph_objects as go\n",
    "import numpy as np\n",
    "\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "\n",
    "import statsmodels.api as sm\n",
    "\n",
    "import sklearn\n",
    "from sklearn import preprocessing\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "from synapse.ml.lightgbm import *\n",
    "\n",
    "pio.renderers.default = \"iframe\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "02b4a9e8-4ea6-4172-b7a0-09db9554b43c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_holdout_sim(index: int,\n",
    "                    sample_cadence: int,\n",
    "                    metric: str,\n",
    "                    n_obs_from_end: int,\n",
    "                    holdout_sim_dfs: List[pd.DataFrame],\n",
    "                    min_max_scaler: MinMaxScaler,\n",
    "                    poly_transform: sklearn.preprocessing.PolynomialFeatures) -> pd.DataFrame:\n",
    "    holdout_sim = holdout_sim_dfs[holdout_index]\n",
    "    holdout_sim = holdout_sim.iloc[[x * sample_cadence for x in range(holdout_sim.shape[0] // sample_cadence)]].copy()\n",
    "\n",
    "    holdout_X = holdout_sim[ivs]\n",
    "    holdout_X = min_max_scaler.transform(holdout_X)\n",
    "    holdout_X = pd.DataFrame(holdout_X, columns=min_max_scaler.get_feature_names_out())\n",
    "    holdout_X = poly_transform.transform(holdout_X)\n",
    "    holdout_X = pd.DataFrame(holdout_X, columns=poly_transform.get_feature_names_out())\n",
    "    holdout_X[metric] = list(holdout_sim[metric])\n",
    "    \n",
    "    return holdout_X.iloc[-n_obs_from_end:].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "cd7247ec-2a88-428e-a90a-353dd44cb55f",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_path = \"/data/saturation/central_composite_design/ccd5\"\n",
    "ivs = [\"slope\", \"effective_radius_multiplier\", \"r_stat_multiplier\", \"min_rim_percentage\", \"n_craters_in_study_region\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "id": "d24096cf-555b-4268-8733-63e89e68272b",
   "metadata": {},
   "outputs": [],
   "source": [
    "stats_df = pd.read_csv(f\"{base_path}/post_saturation_sample_500.csv\")\n",
    "# stats_df = pd.read_csv(f\"{base_path}/post_saturation_sample_5000.csv\")\n",
    "\n",
    "min_max_scaler = MinMaxScaler()\n",
    "stats_df[ivs] = min_max_scaler.fit_transform(stats_df[ivs])\n",
    "\n",
    "paths = list(Path(base_path).glob(\"simulation_*.parquet\"))\n",
    "holdout_sim_dfs = [pd.read_parquet(x) for x in paths]\n",
    "\n",
    "# metric = \"areal_density\"\n",
    "metric = \"za\"\n",
    "# metric = \"z\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e14b73ec-7508-4563-a3b0-34e3120da863",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed4799a8-d7f8-46b2-8970-64e100e70941",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "id": "43e10856-cf79-4408-89e0-421044a4b0eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "poly_transform = sklearn.preprocessing.PolynomialFeatures(degree=2)\n",
    "X = poly_transform.fit_transform(stats_df[ivs])\n",
    "X = pd.DataFrame(X, columns=poly_transform.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "955010c5-2362-4d5d-88cf-90995c14c24a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "id": "f7cc35dc-a619-409a-8dca-3f67e4e53ca6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using RF CIs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "id": "ff3e0399-1175-4085-a76a-8752e4427158",
   "metadata": {},
   "outputs": [],
   "source": [
    "from quantile_forest import RandomForestQuantileRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "id": "b70cf0c0-8fae-49ca-9956-47477cad7102",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = RandomForestQuantileRegressor(n_estimators=500, random_state=42, n_jobs=24)\n",
    "model_fit = model.fit(X, stats_df[metric])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "id": "811702d4-b704-4164-bede-e140ce5e52f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "holdout_index = 5\n",
    "sample_cadence = 250\n",
    "\n",
    "holdout_sim = holdout_sim_dfs[holdout_index]\n",
    "holdout_sim = holdout_sim.iloc[[x * sample_cadence for x in range(holdout_sim.shape[0] // sample_cadence)]].copy()\n",
    "\n",
    "holdout_X = holdout_sim[ivs]\n",
    "holdout_X = min_max_scaler.transform(holdout_X)\n",
    "holdout_X = pd.DataFrame(holdout_X, columns=min_max_scaler.get_feature_names_out())\n",
    "holdout_X = poly_transform.transform(holdout_X)\n",
    "holdout_X = pd.DataFrame(holdout_X, columns=poly_transform.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "id": "d59d5b54-1f6b-4d51-83ff-dc3e76348f43",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model_fit.predict(holdout_X, quantiles=[0.05])\n",
    "predictions = pd.DataFrame(predictions, columns=[\"predicted\"])\n",
    "pred_df = pd.concat([holdout_X, predictions], axis=1)\n",
    "pred_df[\"actual\"] = list(holdout_sim[metric])\n",
    "pred_df[\"n_craters\"] = list(holdout_sim.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "id": "a918533c-e0ff-485f-a434-dd0710d467b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<iframe\n",
       "    scrolling=\"no\"\n",
       "    width=\"100%\"\n",
       "    height=\"545px\"\n",
       "    src=\"iframe_figures/figure_239.html\"\n",
       "    frameborder=\"0\"\n",
       "    allowfullscreen\n",
       "></iframe>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig = go.Figure()\n",
    "\n",
    "fig.add_scatter(x=pred_df[\"n_craters\"],\n",
    "                y=pred_df[\"actual\"],\n",
    "                mode=\"lines\",\n",
    "                name=\"Actual\")\n",
    "fig.add_scatter(x=pred_df[\"n_craters\"],\n",
    "                y=pred_df[\"predicted\"],\n",
    "                mode=\"lines\",\n",
    "                name=\"Lower 5%\")\n",
    "\n",
    "fig.update_layout(\n",
    "    xaxis_title=\"N Craters\",\n",
    "    yaxis_title=metric\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "id": "a19691f1-9245-47f9-b10a-9766a4d7f947",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_cadence = 250\n",
    "n_obs_from_end = 500\n",
    "holdout_dfs = []\n",
    "\n",
    "for index in range(len(holdout_sim_dfs)):\n",
    "    df = get_holdout_sim(index=index,\n",
    "                         sample_cadence=sample_cadence,\n",
    "                         metric=metric,\n",
    "                         n_obs_from_end=n_obs_from_end,\n",
    "                         holdout_sim_dfs=holdout_sim_dfs,\n",
    "                         min_max_scaler=min_max_scaler,\n",
    "                         poly_transform=poly_transform)\n",
    "    holdout_dfs.append(df)\n",
    "\n",
    "all_holdout_df = pd.concat(holdout_dfs, axis=0).reset_index(drop=True)\n",
    "\n",
    "predictions = model_fit.predict(all_holdout_df[[x for x in all_holdout_df.columns if x != metric]], quantiles=[0.05])\n",
    "predictions = pd.DataFrame(predictions, columns=[\"prediction\"])\n",
    "pred_df = pd.concat([all_holdout_df, predictions], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "id": "f00392e3-aa13-48c8-89e1-ad145cda8317",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.232"
      ]
     },
     "execution_count": 241,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Score all holdout sims; check how many observations are below the 5% threshold late into simulation\n",
    "pred_df[pred_df[metric] < pred_df.prediction].shape[0] / pred_df.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "052d4b65-34dc-4d95-8627-0755dfa53d5d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "id": "309d7e1a-9f8f-41d3-9c22-a54ef05476e2",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'y' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [242]\u001b[0m, in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m predictions \u001b[38;5;241m=\u001b[39m model_fit\u001b[38;5;241m.\u001b[39mpredict(X)\n\u001b[1;32m      2\u001b[0m predictions \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(predictions, columns\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprediction\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[0;32m----> 3\u001b[0m pred_df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mconcat([X, \u001b[43my\u001b[49m, predictions], axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'y' is not defined"
     ]
    }
   ],
   "source": [
    "predictions = model_fit.predict(X)\n",
    "predictions = pd.DataFrame(predictions, columns=[\"prediction\"])\n",
    "pred_df = pd.concat([X, stats_df[metric], predictions], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "id": "4d8bb134-d8f7-480c-9e10-5c3098dc0d9b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.12499896953959029"
      ]
     },
     "execution_count": 244,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Score all training sims; check how many observations are below the 5% threshold late into simulation\n",
    "pred_df[pred_df[metric] < pred_df.prediction].shape[0] / pred_df.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53249218-339a-4b3b-93cc-95b1a57db482",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "id": "5ec7b115-b5f2-4272-8eec-ce43ea8794e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql import SparkSession, DataFrame, Row, Window\n",
    "from pyspark.ml.linalg import Vectors\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "n_cores = 24\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "                    .master(f\"local[{n_cores}]\") \\\n",
    "                    .appName(\"Saturation\") \\\n",
    "                    .config(\"spark.driver.memory\", \"48g\") \\\n",
    "                    .config(\"spark.jars.packages\", \"com.microsoft.azure:synapseml_2.12:0.10.2\") \\\n",
    "                    .config(\"spark.jars.repositories\", \"https://mmlspark.azureedge.net/maven\") \\\n",
    "                    .getOrCreate()\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "id": "efa7e911-1253-48ae-99bc-8857f92878a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = spark.createDataFrame(pd.concat([stats_df[metric], X], axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "257338ed-2822-4458-b27d-d8ffbd8b1bcd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "id": "5bf136b6-da89-4757-9177-c1e5b1e14f53",
   "metadata": {},
   "outputs": [],
   "source": [
    "assembler = VectorAssembler(\n",
    "    inputCols=poly_transform.get_feature_names_out(),\n",
    "    outputCol=\"features\")\n",
    "\n",
    "train_df_featurized = assembler.transform(train_df)[metric, \"features\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "id": "f0f3eb83-3e25-4d09-8a44-40e27ad4c287",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/03/05 14:42:15 WARN TaskSetManager: Stage 42 contains a task of very large size (1009 KiB). The maximum recommended task size is 1000 KiB.\n",
      "[LightGBM] [Warning] Find whitespaces in feature_names, replace with underlines\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "model = (LightGBMRegressor()\n",
    "    .setObjective('quantile')\n",
    "    .setLabelCol(metric)\n",
    "    .setAlpha(0.05)\n",
    "    .setLearningRate(0.3)\n",
    "    .setNumIterations(500)\n",
    "    .setNumLeaves(1000)\n",
    "    .fit(train_df_featurized))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "id": "b775f044-0c32-4faf-a7f5-1235b5dc34f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "holdout_index = 5\n",
    "sample_cadence = 250\n",
    "\n",
    "holdout_sim = holdout_sim_dfs[holdout_index]\n",
    "holdout_sim = holdout_sim.iloc[[x * sample_cadence for x in range(holdout_sim.shape[0] // sample_cadence)]].copy()\n",
    "\n",
    "holdout_X = holdout_sim[ivs]\n",
    "holdout_X = min_max_scaler.transform(holdout_X)\n",
    "holdout_X = pd.DataFrame(holdout_X, columns=min_max_scaler.get_feature_names_out())\n",
    "holdout_X = poly_transform.transform(holdout_X)\n",
    "holdout_X = pd.DataFrame(holdout_X, columns=poly_transform.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "id": "b75103ec-11d4-4e02-9e48-4d9d79960bfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "holdout_df = spark.createDataFrame(holdout_X)\n",
    "holdout_df_featurized = assembler.transform(holdout_df)[[\"features\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "id": "68a6aee9-d369-46a8-b8d7-03c48fc69dec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/03/05 14:42:39 WARN DAGScheduler: Broadcasting large task binary with size 44.3 MiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "predictions = model.transform(holdout_df_featurized).toPandas()[\"prediction\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "id": "00dd2ddf-d787-4b0b-a1a1-673e49495b78",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_df = pd.concat([holdout_X, predictions], axis=1)\n",
    "pred_df[\"actual\"] = list(holdout_sim[metric])\n",
    "pred_df[\"n_craters\"] = list(holdout_sim.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "id": "156517ca-76d3-462a-859a-7335ee7fbbfd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<iframe\n",
       "    scrolling=\"no\"\n",
       "    width=\"100%\"\n",
       "    height=\"545px\"\n",
       "    src=\"iframe_figures/figure_253.html\"\n",
       "    frameborder=\"0\"\n",
       "    allowfullscreen\n",
       "></iframe>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig = go.Figure()\n",
    "\n",
    "fig.add_scatter(x=pred_df[\"n_craters\"],\n",
    "                y=pred_df[\"actual\"],\n",
    "                mode=\"lines\",\n",
    "                name=\"Actual\")\n",
    "fig.add_scatter(x=pred_df[\"n_craters\"],\n",
    "                y=pred_df[\"prediction\"],\n",
    "                mode=\"lines\",\n",
    "                name=\"Lower 5%\")\n",
    "\n",
    "fig.update_layout(\n",
    "    xaxis_title=\"N Craters\",\n",
    "    yaxis_title=metric\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d3d04a3-2b75-45cf-b297-b82ee71cab78",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "id": "bd56ddf2-6df3-45be-bbe2-8ab2c6d484ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_cadence = 250\n",
    "n_obs_from_end = 500\n",
    "holdout_dfs = []\n",
    "\n",
    "for index in range(len(holdout_sim_dfs)):\n",
    "    df = get_holdout_sim(index=index,\n",
    "                         sample_cadence=sample_cadence,\n",
    "                         metric=metric,\n",
    "                         n_obs_from_end=n_obs_from_end,\n",
    "                         holdout_sim_dfs=holdout_sim_dfs,\n",
    "                         min_max_scaler=min_max_scaler,\n",
    "                         poly_transform=poly_transform)\n",
    "    holdout_dfs.append(df)\n",
    "\n",
    "all_holdout_df = pd.concat(holdout_dfs, axis=0).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "id": "621b88d7-6492-492b-8ae1-3ba4b4e0d422",
   "metadata": {},
   "outputs": [],
   "source": [
    "holdout_df = spark.createDataFrame(all_holdout_df[[x for x in all_holdout_df.columns if x != metric]])\n",
    "holdout_df_featurized = assembler.transform(holdout_df)[[\"features\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "id": "efc2dfde-5b5b-4564-8a58-3119d14366d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/03/05 14:42:41 WARN DAGScheduler: Broadcasting large task binary with size 44.3 MiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "predictions = model.transform(holdout_df_featurized).toPandas()[\"prediction\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "id": "2d0b9920-042f-4098-8f61-e4d6adf04fcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_df = pd.concat([holdout_df.toPandas(), all_holdout_df[metric], predictions], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "id": "52cfbfc5-6989-4d30-ab17-94c4a9b653f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.09"
      ]
     },
     "execution_count": 258,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Score all holdout sims; check how many observations are below the 5% threshold late into simulation\n",
    "pred_df[pred_df[metric] < pred_df.prediction].shape[0] / pred_df.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de1a2bda-4bfc-4ffb-a1bb-84030f8b2c28",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "id": "7d20fb74-c781-493d-8356-52908b0eccc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/03/05 14:42:43 WARN DAGScheduler: Broadcasting large task binary with size 44.3 MiB\n",
      "23/03/05 14:42:43 WARN TaskSetManager: Stage 46 contains a task of very large size (1009 KiB). The maximum recommended task size is 1000 KiB.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "predictions = model.transform(train_df_featurized).toPandas()[\"prediction\"]\n",
    "pred_df = pd.concat([stats_df[metric], X, predictions], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "id": "d135a2cc-e2ad-4ec9-a086-acbff33c5f41",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.054004369152137174"
      ]
     },
     "execution_count": 260,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Score all training sims; check how many observations are below the 5% threshold late into simulation\n",
    "pred_df[pred_df[metric] < pred_df.prediction].shape[0] / pred_df.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ada28378-1804-4869-a639-faf0dd0d761b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
