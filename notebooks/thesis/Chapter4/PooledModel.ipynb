{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6f6c173c-18cd-475b-9481-248892d582eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5487422c-9a66-4376-9022-7ea9254ca9b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.formula.api as smf\n",
    "from statsmodels.iolib.smpickle import load_pickle\n",
    "from scipy.stats import chi2, norm\n",
    "\n",
    "from saturation.utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2564af7c-2fe9-4b34-ab2b-29d6de1452e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/03/02 11:33:29 WARN Utils: Your hostname, muninn resolves to a loopback address: 127.0.1.1; using 192.168.50.14 instead (on interface enp8s0)\n",
      "25/03/02 11:33:29 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/03/02 11:33:29 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "25/03/02 11:33:30 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n"
     ]
    }
   ],
   "source": [
    "n_cores = 26\n",
    "\n",
    "spark = (\n",
    "    SparkSession.builder\n",
    "    .master(f\"local[{n_cores}]\")\n",
    "    .appName(\"Saturation\")\n",
    "    .config(\"spark.sql.shuffle.partitions\", \"500\")\n",
    "    .config(\"spark.driver.memory\", \"40g\")\n",
    "    .config(\"spark.driver.maxResultSize\", \"16g\")\n",
    "    .getOrCreate()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5d6abd87-9a0e-45f0-918b-4bc5abc6c1a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/03/02 11:33:32 WARN CacheManager: Asked to cache already cached data.\n"
     ]
    }
   ],
   "source": [
    "base_path = \"/data/saturation/thesis_run_20250223/\"\n",
    "\n",
    "colors = [\"blue\", \"black\", \"r\", \"orange\", \"g\"]\n",
    "line_styles = [\"-\"]\n",
    "font_size = 24\n",
    "dpi = 400\n",
    "\n",
    "configs_pdf, configs_df, configs_dict = get_configs(\n",
    "    base_path=base_path,\n",
    "    spark=spark\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc8f5b2e-7f72-4e30-9a01-dd379184af2f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9539ba90-c591-4afa-8480-924c4209c213",
   "metadata": {},
   "source": [
    "# Configuration variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f6205593-dada-441d-967a-99f4f82e3975",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Maximum nstat at which to retrieve statistics\n",
    "MAX_NSTAT_FOR_STATISTICS = int(1e6)\n",
    "\n",
    "# States dataset variables\n",
    "N_NSTATS = 1000\n",
    "MIN_NSTAT = int(2.0e6)\n",
    "MAX_NSTAT = int(2.5e6)\n",
    "\n",
    "# Range of simulation IDs to be used for fitting, inclusive\n",
    "MIN_SIMULATION_ID = 1\n",
    "MAX_SIMULATION_ID = 150\n",
    "\n",
    "# Filter on the simulations to be used for fitting, inclusive\n",
    "MAX_SLOPE = -2.5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb8e1e6d-8490-4e97-9676-323b0839d5de",
   "metadata": {},
   "source": [
    "# Retrieve simulation metadata for simulations with slope <= -2.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5358aa57-91ca-45ee-a4a4-a66430cd0d78",
   "metadata": {},
   "outputs": [],
   "source": [
    "steep_slope_simulation_ids = list(\n",
    "    configs_pdf[configs_pdf.simulation_id.between(MIN_SIMULATION_ID, MAX_SIMULATION_ID) & (configs_pdf.slope <= MAX_SLOPE)].simulation_id\n",
    ")\n",
    "steep_slope_configs_pdf = configs_pdf[configs_pdf.simulation_id.isin(steep_slope_simulation_ids)].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dcbae96-b27a-4552-b47c-f8ab7839987e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ed461dd2-2a87-4729-8b8d-4e2490cffec8",
   "metadata": {},
   "source": [
    "# Prepare and write out data for the pooled model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "910d3333-6cdd-4f2e-8420-09dbb88c53a2",
   "metadata": {},
   "source": [
    "## Write out states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1bf6910a-b90e-47e6-be0a-4362fac72137",
   "metadata": {},
   "outputs": [],
   "source": [
    "step = int((MAX_NSTAT - MIN_NSTAT) / N_NSTATS)\n",
    "nstats = [MIN_NSTAT + x * step for x in range(N_NSTATS)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "783abca3-0e96-44d6-a667-7e1e9b3995e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 991:================================>                   (405 + 26) / 650]\r"
     ]
    }
   ],
   "source": [
    "first_sim_id = steep_slope_simulation_ids[0]\n",
    "study_region_size = configs_dict[first_sim_id][\"study_region_size\"]\n",
    "study_region_padding = configs_dict[first_sim_id][\"study_region_padding\"]\n",
    "\n",
    "for simulation_id in steep_slope_simulation_ids:\n",
    "    stats_df = spark.read.parquet(f\"{base_path}/{simulation_id}/statistics_*.parquet\")\n",
    "    craters_df = spark.read.parquet( f\"{base_path}/{simulation_id}/craters_*.parquet\")\n",
    "    removals_df = spark.read.parquet(f\"{base_path}/{simulation_id}/crater_removals_*.parquet\")\n",
    "    \n",
    "    states = get_states(\n",
    "        stats_df=stats_df,\n",
    "        craters_df=craters_df,\n",
    "        removals_df=removals_df,\n",
    "        nstats=nstats,\n",
    "        study_region_size=study_region_size,\n",
    "        study_region_padding=study_region_padding,\n",
    "        spark=spark,\n",
    "        result_columns=[\"crater_id\", \"radius\", \"nstat\"],\n",
    "    )\n",
    "    states[\"simulation_id\"] = simulation_id\n",
    "    states = states.set_index(\"simulation_id\").sort_index()\n",
    "    states.to_parquet(f\"data/states_{simulation_id}_{N_NSTATS}_{MAX_SLOPE:.2f}.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bc96a46-5d23-48b9-9645-2a3e6c8d2ac2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e0e7fd3c-1200-4dfd-b1e3-735109f87061",
   "metadata": {},
   "source": [
    "## Write out statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd235388-cc7c-4e3a-89cf-00441ce4793a",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_columns = [\n",
    "    \"radius\",\n",
    "    \"lifespan\",\n",
    "    \"simulation_id\"\n",
    "]\n",
    "statistics = get_statistics_with_lifespans_for_simulations(\n",
    "    simulation_ids=steep_slope_simulation_ids,\n",
    "    base_path=base_path,\n",
    "    configs_df=configs_df,\n",
    "    spark=spark,\n",
    "    result_columns=result_columns,\n",
    "    max_nstat=MAX_NSTAT_FOR_STATISTICS,\n",
    ")\n",
    "statistics = statistics.set_index(\"simulation_id\").sort_index()\n",
    "statistics.to_parquet(\"data/statistics_{MAX_SLOPE:.2f}.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4fbc23a-ae29-4dcd-9784-5cd49d1611df",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6f47ac3b-7b60-43b8-9a9d-f5ba7861879b",
   "metadata": {},
   "source": [
    "# Fit the pooled model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "100acd7f-369b-4a43-b7fc-12511fe38214",
   "metadata": {},
   "source": [
    "## Load the statistics data back from disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3209606-5ba1-481f-ab0c-2a7cdf62dcaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "statistics = pd.read_parquet(\"data/statistics_{MAX_SLOPE:.2f}.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b03f5b1-9355-4598-8904-55f068f92035",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e1ff3889-4c1b-4e9f-8122-aacbf6a7381f",
   "metadata": {},
   "source": [
    "## Fit the model, write out to disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01ecad08-36f7-4fab-a891-4a5ea157ed74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample the data; 10% is nearly the max that can fit into memory\n",
    "statistics_sample = statistics.sample(frac=0.1)\n",
    "\n",
    "model_formula = (\n",
    "    \"lifespan ~ 1 \"\n",
    "    \"+ slope:rim_erasure_exponent:np.log(radius) \"\n",
    "    \"+ rim_erasure_exponent:np.log(radius) \"\n",
    "    \"+ np.log(-slope)\"\n",
    ")\n",
    "\n",
    "lifespan_model = smf.negativebinomial(\n",
    "    data=statistics_sample,\n",
    "    formula=model_formula\n",
    ").fit(\n",
    "    maxiter=1000,\n",
    "    method=\"BFGS\"\n",
    ")\n",
    "lifespan_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06a606d3-870c-40ea-bb61-e8d62c7a4a84",
   "metadata": {},
   "outputs": [],
   "source": [
    "lifespan_model.save(\"data/pooled_lifespan_model_{MAX_SLOPE:.2f}.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01af6fc4-03d9-47b6-b56c-7e9945dee92f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2596f584-ecba-4f93-9289-e3728036b535",
   "metadata": {},
   "source": [
    "# Create the prediction dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e04d5be-6d96-4b01-b0f6-ae75fd4846d5",
   "metadata": {},
   "source": [
    "## Reload the model from disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc9e6a3f-87e6-4796-a7ed-d33f6912c324",
   "metadata": {},
   "outputs": [],
   "source": [
    "lifespan_model = load_pickle(\"data/pooled_lifespan_model_{MAX_SLOPE:.2f}.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e516a62-dfe6-4012-91e0-6f6e3181192b",
   "metadata": {},
   "outputs": [],
   "source": [
    "lifespan_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9985da2a-296d-4898-a63e-a99d612d1f93",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "34298b19-94f2-419b-879f-e5cf9fafd40e",
   "metadata": {},
   "source": [
    "## Reload statistics from disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6248e628-cea2-454a-9a2b-60fc064cfa92",
   "metadata": {},
   "outputs": [],
   "source": [
    "statistics = pd.read_parquet(\"data/statistics_{MAX_SLOPE:.2f}.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe43c20a-8db0-4345-814c-2b8eda099e01",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a33546ae-d796-43c1-9386-d914a136e79b",
   "metadata": {},
   "source": [
    "## Predict using Little's Law for all simulations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48bd7a7b-dc45-4dbc-b68b-08c1276b3049",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract coefficients\n",
    "slope_exponent_radius_coeff = lifespan_model.params[\"slope:rim_erasure_exponent:np.log(radius)\"]\n",
    "exponent_radius_coeff = lifespan_model.params[\"rim_erasure_exponent:np.log(radius)\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7831273-e97b-4336-8a9f-83d31ece2952",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct Little's Law model predictions for each simulation\n",
    "predictions = pd.DataFrame(steep_slope_simulation_ids, columns=[\"simulation_id\"])\n",
    "predictions[\"slope\"] = [configs_dict[x][\"slope\"] for x in steep_slope_simulation_ids]\n",
    "predictions[\"rim_erasure_exponent\"] = [configs_dict[x][\"rim_erasure_method\"][\"exponent\"] for x in steep_slope_simulation_ids]\n",
    "predictions[\"lifespan_model_exponent\"] = (\n",
    "    slope_exponent_radius_coeff * predictions[\"slope\"] * predictions[\"rim_erasure_exponent\"]\n",
    "    + exponent_radius_coeff * predictions[\"rim_erasure_exponent\"]\n",
    ")\n",
    "predictions[\"littles_law_model_slope_prediction\"] = predictions.slope + predictions.lifespan_model_exponent\n",
    "predictions.set_index(\"simulation_id\", inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f65f694c-f7d7-419e-abd5-b4f649b07c0d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7c516539-ed4b-44bc-a097-a6081023cb05",
   "metadata": {},
   "source": [
    "## Estimate slopes for each simulation using MLE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d87e401-7882-4980-9b63-de81b7e1f899",
   "metadata": {},
   "source": [
    "### Load states data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "930de680-6bd5-4190-ba13-b5b72e70be30",
   "metadata": {},
   "outputs": [],
   "source": [
    "states_sub_dfs = []\n",
    "for simulation_id in steep_slope_simulation_ids:\n",
    "    state = pd.read_parquet(f\"data/states_{simulation_id}_{N_NSTATS}_{MAX_SLOPE:.2f}.parquet\")\n",
    "    states_sub_dfs.append(state)\n",
    "states = pd.concat(states_sub_dfs, axis=0)\n",
    "del states_sub_dfs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7fd9201-d8f0-4fd3-9993-b2d00e52c599",
   "metadata": {},
   "source": [
    "### Estimate MLE slope and sigma for each simulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faf5815f-0775-4038-8907-af3fe980738a",
   "metadata": {},
   "outputs": [],
   "source": [
    "for simulation_id in steep_slope_simulation_ids:\n",
    "    state = states.loc[simulation_id]\n",
    "    mle_slope, sigma = estimate_cumulative_slope(\n",
    "        radii=state.radius,\n",
    "        rmin=configs_dict[simulation_id][\"rstat\"],\n",
    "        rmax=configs_dict[simulation_id][\"rmax\"],\n",
    "        min_search_slope=-10.0,\n",
    "        max_search_slope=-1\n",
    "    )\n",
    "    predictions.loc[simulation_id, \"mle_slope\"] = mle_slope\n",
    "    predictions.loc[simulation_id, \"mle_slope_sigma\"] = sigma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e58e278-aa10-4ecd-9ad2-1f03dbb0010e",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3461a38b-94a9-4513-85d9-1d937b911f16",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "dfe6edd0-0af4-4a5d-953c-516814ac0cc1",
   "metadata": {},
   "source": [
    "## Save off predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e75a193d-ee16-4177-bd23-705f18ef6a9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions.to_parquet(\"data/predictions_{MAX_SLOPE:.2f}.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "047a35d0-8710-4277-9b69-e273ecb2b9db",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "fb4b24c2-7ffe-4e2a-8cdd-ea721dfa184d",
   "metadata": {},
   "source": [
    "# Perform hypothesis testing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00d64530-ee55-4c85-a876-68e4042176d9",
   "metadata": {},
   "source": [
    "## Reload predictions from disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ad7b8a6-8b7d-4f34-bee1-9f7abfcb3b12",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = pd.read_parquet(\"data/predictions_{MAX_SLOPE:.2f}.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8043607b-c657-4cc8-ba6a-0e8fdd80e438",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6ed90cca-7e08-4280-b4a0-5ccff7bc5be5",
   "metadata": {},
   "source": [
    "## Perform the TOST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f5fd3d5-ce3e-4b04-b517-7214f2bd604f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tost_equivalence_test(\n",
    "    *,\n",
    "    mle_slope: float,\n",
    "    mle_slope_sigma: float,\n",
    "    predicted_slope: float,\n",
    "    margin: float\n",
    ") -> float:\n",
    "    \"\"\"\n",
    "    Perform a TOST (Two One-Sided Test) equivalence check for a single simulation.\n",
    "\n",
    "    Null hypothesis (H0): The true slope is outside ±margin of predicted_slope.\n",
    "    Alternative (H1): The true slope is within ±margin of predicted_slope.\n",
    "\n",
    "    This function returns a single p-value for the equivalence test,\n",
    "    following the approach of taking the maximum of the two one-sided p-values\n",
    "    (i.e., p_equiv = max(p1, p2)).\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    mle_slope : float\n",
    "        MLE estimate of the slope for this simulation.\n",
    "    mle_slope_sigma : float\n",
    "        Standard error of the MLE slope estimate.\n",
    "    predicted_slope : float\n",
    "        The predicted slope from the model (Little's Law).\n",
    "    margin : float\n",
    "        Equivalence margin (e.g., ±0.05).\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    p_equiv : float\n",
    "        A single p-value for the TOST equivalence test.\n",
    "        Typically compared to alpha (e.g., 0.05).\n",
    "        A smaller value indicates stronger evidence of equivalence.\n",
    "    \"\"\"\n",
    "    # Two one-sided tests:\n",
    "    #  1) slope > predicted_slope - margin\n",
    "    #  2) slope < predicted_slope + margin\n",
    "\n",
    "    z1 = (\n",
    "        mle_slope\n",
    "        - (predicted_slope - margin)\n",
    "    ) / mle_slope_sigma\n",
    "    p1 = 1.0 - norm.cdf(z1)\n",
    "\n",
    "    z2 = (\n",
    "        (predicted_slope + margin)\n",
    "        - mle_slope\n",
    "    ) / mle_slope_sigma\n",
    "    p2 = 1.0 - norm.cdf(z2)\n",
    "\n",
    "    # A single TOST p-value is often the max of these two p-values.\n",
    "    p_equiv = max(p1, p2)\n",
    "\n",
    "    return p_equiv\n",
    "\n",
    "\n",
    "def fishers_method(\n",
    "    p_values: list[float]\n",
    ") -> tuple[float, float]:\n",
    "    \"\"\"\n",
    "    Combine a list of p-values using Fisher's method.\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    chi2_stat : float\n",
    "        The combined chi-square statistic.\n",
    "    combined_pval : float\n",
    "        p-value for the combined test.\n",
    "    \"\"\"\n",
    "    valid_pvals = [\n",
    "        p for p in p_values\n",
    "        if 0.0 < p < 1.0\n",
    "    ]\n",
    "\n",
    "    if not valid_pvals:\n",
    "        # If no valid p-values, return defaults\n",
    "        return 0.0, 1.0\n",
    "\n",
    "    chi2_stat = -2.0 * np.sum(\n",
    "        np.log(valid_pvals)\n",
    "    )\n",
    "    df = 2 * len(valid_pvals)\n",
    "    combined_pval = 1.0 - chi2.cdf(\n",
    "        chi2_stat,\n",
    "        df\n",
    "    )\n",
    "\n",
    "    return chi2_stat, combined_pval\n",
    "\n",
    "\n",
    "def run_equivalence_testing(\n",
    "    *,\n",
    "    df: pd.DataFrame,\n",
    "    margin: float,\n",
    "    alpha: float\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Main routine to run TOST per simulation, then apply Fisher's method\n",
    "    to combine p-values for an overall conclusion.\n",
    "\n",
    "    The DataFrame `df` must contain columns:\n",
    "      - \"littles_law_model_slope_prediction\"\n",
    "      - \"mle_slope\"\n",
    "      - \"mle_slope_sigma\"\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    df : pd.DataFrame\n",
    "        DataFrame with required columns, indexed by simulation_id or similar.\n",
    "    margin : float\n",
    "        Equivalence margin (e.g., ±0.05).\n",
    "    alpha : float\n",
    "        Significance level for TOST (e.g., 0.05).\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    results_df : pd.DataFrame\n",
    "        A DataFrame with:\n",
    "          - \"predicted_slope\"\n",
    "          - \"predicted_lower_ci\"\n",
    "          - \"predicted_upper_ci\"\n",
    "          - \"mle_slope\"\n",
    "          - \"mle_slope_sigma\"\n",
    "          - \"mle_lower_ci\"\n",
    "          - \"mle_upper_ci\"\n",
    "          - \"p_tost\"\n",
    "          - \"equivalent\"\n",
    "        The function prints a Fisher combined p-value for the entire set.\n",
    "    \"\"\"\n",
    "    # z-critical value for the (1 - 2*alpha)% CI\n",
    "    # e.g., alpha=0.05 => 1 - 2*0.05=0.90 => z ~1.645\n",
    "    z_crit = norm.ppf(1.0 - alpha)\n",
    "\n",
    "    p_values = []\n",
    "    dfs_list = []\n",
    "\n",
    "    for idx, row in df.iterrows():\n",
    "        predicted_slope = row[\"littles_law_model_slope_prediction\"]\n",
    "        est_slope = row[\"mle_slope\"]\n",
    "        est_sigma = row[\"mle_slope_sigma\"]\n",
    "\n",
    "        # 1) TOST p-value\n",
    "        p_tost = tost_equivalence_test(\n",
    "            mle_slope=est_slope,\n",
    "            mle_slope_sigma=est_sigma,\n",
    "            predicted_slope=predicted_slope,\n",
    "            margin=margin\n",
    "        )\n",
    "\n",
    "        # 2) Equivalence pass/fail\n",
    "        equivalent = (p_tost < alpha)\n",
    "\n",
    "        # 3) Confidence Interval for predicted slope: simply ± margin\n",
    "        predicted_lower_ci = predicted_slope - margin\n",
    "        predicted_upper_ci = predicted_slope + margin\n",
    "\n",
    "        # 4) (1 - 2*alpha)% CI for MLE slope\n",
    "        # e.g. for alpha=0.05 => 90% CI\n",
    "        mle_lower_ci = est_slope - z_crit * est_sigma\n",
    "        mle_upper_ci = est_slope + z_crit * est_sigma\n",
    "\n",
    "        p_values.append(p_tost)\n",
    "        dfs_list.append({\n",
    "            \"simulation_id\": idx,\n",
    "            \"predicted_slope\": predicted_slope,\n",
    "            \"predicted_lower_ci\": predicted_lower_ci,\n",
    "            \"predicted_upper_ci\": predicted_upper_ci,\n",
    "            \"mle_slope\": est_slope,\n",
    "            \"mle_slope_sigma\": est_sigma,\n",
    "            \"mle_lower_ci\": mle_lower_ci,\n",
    "            \"mle_upper_ci\": mle_upper_ci,\n",
    "            \"p_tost\": p_tost,\n",
    "            \"equivalent\": equivalent\n",
    "        })\n",
    "\n",
    "    results_df = pd.DataFrame(dfs_list).set_index(\"simulation_id\")\n",
    "\n",
    "    # Combine p-values via Fisher\n",
    "    chi2_stat, combined_pval = fishers_method(p_values)\n",
    "\n",
    "    dof = 2 * len(p_values)\n",
    "    print(f\"Fisher Combined chi-square = {chi2_stat:.4f} with dof={dof}\")\n",
    "    print(f\"Fisher Combined p-value = {combined_pval:.6g}\")\n",
    "    print(\"Conclusion: If combined_p-value < alpha, the model's predicted slope is strongly supported across all simulations.\")\n",
    "\n",
    "    return results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d7b7212-50de-46e1-bcb6-96d48b0bcd5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "equivalence_test_results = run_equivalence_testing(\n",
    "    df=predictions,\n",
    "    alpha=0.05,\n",
    "    margin=0.05\n",
    ")\n",
    "\n",
    "result_cols = [\n",
    "    \"mle_lower_ci\",\n",
    "    \"mle_upper_ci\",\n",
    "    \"predicted_lower_ci\",\n",
    "    \"predicted_upper_ci\",\n",
    "    \"p_tost\",\n",
    "    \"equivalent\"\n",
    "]\n",
    "predictions[result_cols] = equivalence_test_results[result_cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76fce71a-f5ee-4cb7-895b-c5fe2d690165",
   "metadata": {},
   "outputs": [],
   "source": [
    "equivalence_test_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f19adf6-16f0-4823-8684-9efe2253c211",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions[~predictions.equivalent].sort_values([\"slope\", \"rim_erasure_exponent\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17eca42a-d204-40c3-a006-5f680908348a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
