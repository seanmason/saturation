{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d3334338-0b18-49cb-9446-d8ab8ed50ae8",
   "metadata": {},
   "source": [
    "import sys\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import yaml\n",
    "from typing import *\n",
    "\n",
    "import plotly.express as px\n",
    "import plotly.io as pio\n",
    "import plotly.graph_objects as go\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import pyspark\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql import SparkSession, DataFrame, Window\n",
    "from hyperopt import hp, fmin, tpe, STATUS_OK, Trials, space_eval\n",
    "import itertools\n",
    "\n",
    "from pyspark.ml.feature import VectorAssembler, MinMaxScaler, BucketedRandomProjectionLSH\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.functions import vector_to_array\n",
    "\n",
    "from utils import *\n",
    "\n",
    "pio.renderers.default = \"iframe\""
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a39631b-1cce-4bf7-a33a-3d3a957ea945",
   "metadata": {},
   "source": [],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a1933719-77f6-4b6a-8120-842670918528",
   "metadata": {},
   "source": [
    "def quantize_value(value: float, delta: float) -> float:\n",
    "    return np.round(int(np.round(value / delta, decimals=5)) * delta, decimals=5)\n",
    "\n",
    "\n",
    "def get_min_max_n_buckets(column: str, delta: float, df) -> Tuple[float, float, int]:\n",
    "    min_max_df = df.select(F.min(column), F.max(column)).toPandas()\n",
    "    \n",
    "    min_val = quantize_value(min_max_df.iloc[0, 0], delta)\n",
    "    max_val = quantize_value(min_max_df.iloc[0, 1], delta)\n",
    "    n_buckets = int(np.round((max_val - min_val) / delta, decimals=5)) + 1\n",
    "\n",
    "    return min_val, max_val, n_buckets\n",
    "\n",
    "def get_confidence_intervals(train: DataFrame,\n",
    "                             test: DataFrame,\n",
    "                             predictor_variables: List[str],\n",
    "                             bandwidths: pd.DataFrame,\n",
    "                             spark_session: SparkSession) -> pd.DataFrame:\n",
    "    train.createOrReplaceTempView(\"train\")\n",
    "    test.createOrReplaceTempView(\"test\")\n",
    "    \n",
    "    bandwidths_df = spark_session.createDataFrame(bandwidths).cache()\n",
    "    bandwidths_df.createOrReplaceTempView(\"bandwidths\")\n",
    "\n",
    "    # Trick it into caching and broadcasting\n",
    "    bandwidths_df.count()\n",
    "\n",
    "    simulation_id_present = \"simulation_id\" in test.columns\n",
    "    \n",
    "    group_by_clause = \",\\n \".join([f\"te.{x}\" for x in predictor_variables] + [f\"b.{x}\" for x in bandwidths.columns])\n",
    "\n",
    "    if \"target\" in test.columns:\n",
    "        group_by_clause += \"\\n, te.target\"\n",
    "        target_select_clause = \"te.target AS target,\"\n",
    "    else:\n",
    "        target_select_clause = \"\"\n",
    "        \n",
    "    if simulation_id_present:\n",
    "        simulation_id_clause = \"AND tr.simulation_id <> te.simulation_id\\n\"\n",
    "        bandwidths_select_clause = \"te.simulation_id AS simulation_id, te.crater_id AS crater_id,\\n\"\n",
    "        group_by_clause += \"\\n, te.simulation_id, te.crater_id\"\n",
    "    else:\n",
    "        simulation_id_clause = \"\"\n",
    "        bandwidths_select_clause = \"\"\n",
    "\n",
    "\n",
    "    test_select_clause = \"\"\n",
    "    bandwidth_clauses = \"1=1 \"\n",
    "    for var in predictor_variables:\n",
    "        test_select_clause += f\"te.{var} as test_{var},\\n\"\n",
    "        if var in bandwidths.columns:\n",
    "            bandwidth_clauses += f\"AND tr.{var} BETWEEN (te.{var} - b.{var}) AND (te.{var} + b.{var})\\n\"\n",
    "\n",
    "    for var in predictor_variables:\n",
    "        bandwidths_select_clause += f\"b.{var} as bandwidth_{var},\\n\"\n",
    "\n",
    "    query = f\"\"\"\n",
    "    SELECT\n",
    "        {test_select_clause}\n",
    "        {bandwidths_select_clause}\n",
    "        {target_select_clause}\n",
    "        mean(tr.target) as target_mean,\n",
    "        count(tr.target) as n_obs,\n",
    "        count(distinct tr.simulation_id) as n_unique_sims,\n",
    "        approx_percentile(tr.target, array(0.025, 0.975), 5000) as percentiles\n",
    "    FROM\n",
    "        test te\n",
    "        INNER JOIN bandwidths b\n",
    "        LEFT JOIN train tr\n",
    "            ON {bandwidth_clauses}\n",
    "    WHERE\n",
    "        1=1\n",
    "        {simulation_id_clause}\n",
    "    GROUP BY\n",
    "        {group_by_clause}\n",
    "    \"\"\"\n",
    "    result = spark.sql(query)\n",
    "\n",
    "    return result.toPandas()\n",
    "\n",
    "\n",
    "def create_objective_function(train: DataFrame,\n",
    "                              test: DataFrame,\n",
    "                              predictor_variables: List[str],\n",
    "                              spark: SparkSession):\n",
    "    \"\"\"\n",
    "    Creates an objective function for HyperOpt optimization\n",
    "    \"\"\"\n",
    "    def objective_function(args):\n",
    "        min_obs = 100\n",
    "        n_too_many_obs = 500000\n",
    "        \n",
    "        bandwidths_data = [[args[f\"bandwidth_{x}\"] for x in predictor_variables]]\n",
    "        bandwidths = pd.DataFrame(bandwidths_data, columns=predictor_variables)\n",
    "        \n",
    "        result = get_confidence_intervals(train, test, predictor_variables, bandwidths, spark)\n",
    "\n",
    "        ci_high = result.percentiles.map(lambda x: x[-1])\n",
    "        ci_low = result.percentiles.map(lambda x: x[0])\n",
    "        orders_of_magnitude_rms = np.sqrt((np.log10(ci_high / ci_low) ** 2).mean())\n",
    "        orders_of_magnitude = np.log10(ci_high / ci_low).mean()\n",
    "        orders_of_magnitude_stdev = np.log10(ci_high / ci_low).std()\n",
    "        mean_n_unique_sims = result.n_unique_sims.mean()\n",
    "        mean_n_obs = result.n_obs.mean()\n",
    "        \n",
    "        percent_inside_ci = ((result.target >= ci_low) & (result.target <= ci_high)).mean()\n",
    "\n",
    "        # Penalize if more or less than 5% are outside of the CI\n",
    "        deviance_from_95 = np.abs(0.95 - percent_inside_ci)\n",
    "        deviance_loss_multiplier = 1.0 if deviance_from_95 < 0.01 else 1 + deviance_from_95 * 10.0\n",
    "        \n",
    "        # Penalize for the fraction with too few observations\n",
    "        too_few = result.n_obs[(result.n_obs < min_obs) | (result.n_obs.isna())]\n",
    "        n_obs_loss_multiplier = 1 + too_few.mean() * 100 if too_few.shape[0] > 0 else 1.0\n",
    "\n",
    "        # Penalize for the fraction with too many observations\n",
    "        too_many = result.n_obs > n_too_many_obs\n",
    "        n_too_many_obs_loss_multiplier = 1 + too_many.mean() * 10 if too_many.shape[0] > 0 else 1.0\n",
    "\n",
    "        loss = orders_of_magnitude_rms * deviance_loss_multiplier * n_obs_loss_multiplier * n_too_many_obs_loss_multiplier\n",
    "\n",
    "        print(f\"{args}, {loss:.3f}, {orders_of_magnitude:.4f}, {orders_of_magnitude_stdev:.4f}, {orders_of_magnitude_rms:.4f}, {n_obs_loss_multiplier:.4f}, {n_too_many_obs_loss_multiplier:.4f}, {deviance_loss_multiplier:.4f}, {mean_n_unique_sims}, {mean_n_obs:.4f}, \")\n",
    "        \n",
    "        return {\n",
    "            \"loss\": loss,\n",
    "            \"params\": args,\n",
    "            \"orders_of_magnitude\": orders_of_magnitude,\n",
    "            \"orders_of_magnitude_rms\": orders_of_magnitude_rms,\n",
    "            \"n_obs_loss_multiplier\": n_obs_loss_multiplier,\n",
    "            \"deviance_loss_multiplier\": deviance_loss_multiplier,\n",
    "            \"mean_n_unique_sims\": mean_n_unique_sims,\n",
    "            \"mean_n_obs\": mean_n_obs,\n",
    "            \"status\": STATUS_OK\n",
    "        }\n",
    "        \n",
    "    return objective_function"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "035cd917-18a8-46c6-a6fe-cd08a012d61e",
   "metadata": {},
   "source": [],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "674a1535-222f-4f21-9e00-8dd650362970",
   "metadata": {},
   "source": [
    "n_cores = 26\n",
    "\n",
    "spark = (SparkSession.builder\n",
    "         .master(f\"local[{n_cores}]\")\n",
    "         .appName(\"Saturation\")\n",
    "         .config(\"spark.sql.shuffle.partitions\", \"500\")\n",
    "         .config(\"spark.driver.memory\", \"60g\")\n",
    "         .config(\"spark.driver.maxResultSize\", \"8g\")\n",
    "         .getOrCreate())"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5cdbe8f-09b3-4bf1-82bf-fbbaa157647b",
   "metadata": {},
   "source": [],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "f0f6724c-7039-43d4-948f-123540f2297b",
   "metadata": {},
   "source": [
    "## Prepare and save the dataset to disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "81651d5a-5a03-488f-9481-e7c4f4327a3b",
   "metadata": {},
   "source": [
    "# base_path = \"/data/saturation/n_craters_stop_condition_20230918\"\n",
    "base_path = \"/data/saturation/thesis_run_20240130\""
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "97d264bf-db70-49ab-85fb-5c2faf57076d",
   "metadata": {},
   "source": [
    "r_stat = 5\n",
    "# study_region_size = 4000 * 4000 / r_stat ** 2\n",
    "study_region_size = 2000 ** 2 / r_stat ** 2\n",
    "\n",
    "configs_df = create_configs_df(read_configs(base_path, spark))\n",
    "data = spark.read.parquet(f\"{base_path}/*/statistics_*.parquet\")\n",
    "\n",
    "# State c2c nn dist in terms of r_stat and log-scale\n",
    "data = data.select(\n",
    "    \"*\",\n",
    "    F.log10(F.col(\"center_to_center_nearest_neighbor_distance_mean\") / F.lit(r_stat)).alias(\"log_mean_c2c_nn_dist\"),\n",
    "    F.log10(\"areal_density\").alias(\"log_ad\"),\n",
    "    (F.col(\"n_craters_in_study_region\") / F.col(\"n_craters_added_in_study_region\")).alias(\"information_remaining\")\n",
    ")"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7659707-1ceb-4961-8e42-013672b70eac",
   "metadata": {},
   "source": [],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "23426a80-2c74-4b11-ba4d-4ad97340e8a5",
   "metadata": {},
   "source": [
    "target = \"n_craters_added_in_study_region\"\n",
    "predictor_variables = [\n",
    "    \"slope\",\n",
    "    \"log_mean_c2c_nn_dist\",\n",
    "    \"log_ad\"\n",
    "]"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2c882c23-45eb-479f-aeca-ea13aedb0eab",
   "metadata": {},
   "source": [
    "params_sets = [\n",
    "    # {\n",
    "    #     \"n_test_samples\": 1000,\n",
    "    #     \"train_sample_fraction\": 0.05,\n",
    "    # },\n",
    "    # {\n",
    "    #     \"n_test_samples\": int(1e9),\n",
    "    #     \"train_sample_fraction\": 0.05,\n",
    "    # },\n",
    "    # {\n",
    "    #     \"n_test_samples\": int(1e9),\n",
    "    #     \"train_sample_fraction\": 0.25,\n",
    "    # },\n",
    "    # {\n",
    "    #     \"n_test_samples\": int(1e9),\n",
    "    #     \"train_sample_fraction\": 1.00,\n",
    "    # },\n",
    "    # {\n",
    "    #     \"n_test_samples\": int(1e9),\n",
    "    #     \"train_sample_fraction\": 1.00,\n",
    "    # },\n",
    "    {\n",
    "        \"n_test_samples\": int(1e9),\n",
    "        \"train_sample_fraction\": 0.25,\n",
    "    },\n",
    "    # {\n",
    "    #     \"n_test_samples\": 1000,\n",
    "    #     \"train_sample_fraction\": 0.1,\n",
    "    # },\n",
    "    # {\n",
    "    #     \"n_test_samples\": 1000,\n",
    "    #     \"train_sample_fraction\": 0.25,\n",
    "    # },\n",
    "    # {\n",
    "    #     \"n_test_samples\": 1000,\n",
    "    #     \"train_sample_fraction\": 1.0,\n",
    "    # },\n",
    "    # {\n",
    "    #     \"n_test_samples\": 5000,\n",
    "    #     \"train_sample_fraction\": 0.05,\n",
    "    # },\n",
    "    # {\n",
    "    #     \"n_test_samples\": 5000,\n",
    "    #     \"train_sample_fraction\": 0.1,\n",
    "    # },\n",
    "    # {\n",
    "    #     \"n_test_samples\": 5000,\n",
    "    #     \"train_sample_fraction\": 0.25,\n",
    "    # },\n",
    "    # {\n",
    "    #     \"n_test_samples\": 10000,\n",
    "    #     \"train_sample_fraction\": 0.05,\n",
    "    # },\n",
    "    # {\n",
    "    #     \"n_test_samples\": 10000,\n",
    "    #     \"train_sample_fraction\": 0.1,\n",
    "    # },\n",
    "    # {\n",
    "    #     \"n_test_samples\": 10000,\n",
    "    #     \"train_sample_fraction\": 0.25,\n",
    "    # },\n",
    "    # {\n",
    "    #     \"n_test_samples\": 10000,\n",
    "    #     \"train_sample_fraction\": 0.1,\n",
    "    # },\n",
    "    # {\n",
    "    #     \"n_test_samples\": 10000,\n",
    "    #     \"train_sample_fraction\": 1,\n",
    "    # },\n",
    "]\n",
    "\n",
    "data_subset = data.where(\n",
    "    (F.col(\"n_craters_added_in_study_region\") > F.lit(50))\n",
    ")\n",
    "\n",
    "for params in params_sets:\n",
    "    train_sample_fraction = params[\"train_sample_fraction\"]\n",
    "    n_test_samples = params[\"n_test_samples\"]\n",
    "\n",
    "    train, test = setup_datasets_for_model(\n",
    "        data_subset,\n",
    "        configs_df,\n",
    "        0.1,\n",
    "        predictor_variables,\n",
    "        target,\n",
    "        train_sample_fraction,\n",
    "        n_test_samples,\n",
    "        spark,\n",
    "        cache_train=False,\n",
    "        cache_test=False\n",
    "    )\n",
    "\n",
    "    train.coalesce(500).write.format(\"parquet\").mode(\"overwrite\").save(f\"data/train_{train_sample_fraction:.3f}_{n_test_samples}.parquet\")\n",
    "    test.coalesce(50).write.format(\"parquet\").mode(\"overwrite\").save(f\"data/test_{train_sample_fraction:.3f}_{n_test_samples}.parquet\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "0000b1e4-554e-43f6-a2e7-bee519160ab7",
   "metadata": {},
   "source": [
    "## Calibrating of bandwidths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0eaea1a3-07b9-4ef8-92b1-7c1dd6a8bfc3",
   "metadata": {},
   "source": [
    "target = \"n_craters_added_in_study_region\"\n",
    "predictor_variables = [\n",
    "    \"slope\",\n",
    "    \"log_mean_c2c_nn_dist\",\n",
    "    \"log_ad\"\n",
    "]\n",
    "\n",
    "# Force caching of train and test data, as they are small\n",
    "train = spark.read.parquet(\"data/train_0.050_1000.parquet\").cache()\n",
    "train.count()\n",
    "\n",
    "test = spark.read.parquet(\"data/test_0.050_1000.parquet\").cache()\n",
    "test.count()\n",
    "\n",
    "trials = Trials()\n",
    "space = {\n",
    "    \"bandwidth_slope\": hp.uniform(\"bandwidth_slope\", 0.05, 0.3),\n",
    "    \"bandwidth_log_mean_c2c_nn_dist\": hp.uniform(\"bandwidth_log_mean_c2c_nn_dist\", 0.0005, 0.2),\n",
    "    \"bandwidth_log_ad\": hp.uniform(\"bandwidth_log_ad\", 0.001, 0.3),\n",
    "}\n",
    "\n",
    "best = fmin(create_objective_function(train, test, predictor_variables, spark),\n",
    "            space=space,\n",
    "            algo=tpe.suggest,\n",
    "            trials=trials,\n",
    "            max_evals=500)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abfeca1c-e59c-4a48-b9b6-64962608ed2c",
   "metadata": {},
   "source": [
    "trials"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26e68f2f-bd24-4135-92a7-1afe754401e4",
   "metadata": {},
   "source": [
    "\"\"\"\n",
    "Oddly good losses for 0.050_1000:\n",
    "{'bandwidth_log_ad': 0.09507393547881537, 'bandwidth_log_mean_c2c_nn_dist': 0.004143332056813422, 'bandwidth_slope': 0.2}, 5.547, 1.4063, 0.3337, 1.4453, 1.0000, 3.8381, 1.0000, 972.1454918032787, 380108.4395, \n",
    "{'bandwidth_log_ad': 0.0734955411086102, 'bandwidth_log_mean_c2c_nn_dist': 0.0038103947728301826, 'bandwidth_slope': 0.2}, 3.836, 1.4028, 0.3352, 1.4422, 1.0000, 2.6598, 1.0000, 878.1967213114754, 299167.2951,     \n",
    "\n",
    "For 0.100_1000:\n",
    "{'bandwidth_log_ad': 0.10846260454421022, 'bandwidth_log_mean_c2c_nn_dist': 0.0018803208882686748, 'bandwidth_slope': 0.2}, 6.136, 1.3785, 0.3344, 1.4185, 1.0000, 4.3260, 1.0000, 991.5437636761488, 398640.2495,           \n",
    "\"\"\""
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5551815-318b-47b9-a3eb-2b2d8717a2b1",
   "metadata": {},
   "source": [],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "c1c1d268-ba18-47f4-a4b2-c7c9deb761d3",
   "metadata": {},
   "source": [
    "## Scoring Dione Surfaces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f36e5898-2030-43b5-a999-fa929db893bf",
   "metadata": {},
   "source": [
    "# train = spark.read.parquet(\"data/train_0.250_1000.parquet\")\n",
    "# train = spark.read.parquet(\"data/train_1.000_1000.parquet\")\n",
    "# train = spark.read.parquet(f\"data/train_1.000_{int(1e9)}.parquet\")\n",
    "train = spark.read.parquet(f\"data/train_0.250_{int(1e9)}.parquet\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ff1f0318-add4-4cf1-b89c-ef5fb1726223",
   "metadata": {},
   "source": [
    "target = \"n_craters_added_in_study_region\"\n",
    "predictor_variables = [\n",
    "    \"slope\",\n",
    "    \"log_mean_c2c_nn_dist\",\n",
    "    \"log_ad\"\n",
    "]\n",
    "\n",
    "observation_dee = {\n",
    "    \"slope\": 2.10,\n",
    "    \"log_mean_c2c_nn_dist\": 0.8136,\n",
    "    \"log_ad\": np.log10(0.06)\n",
    "}\n",
    "\n",
    "observation_dsp = {\n",
    "    \"slope\": 2.66,\n",
    "    \"log_mean_c2c_nn_dist\": 0.6685,\n",
    "    \"log_ad\": np.log10(0.13)\n",
    "}\n",
    "\n",
    "observation_dicp = {\n",
    "    \"slope\": 2.08,\n",
    "    \"log_mean_c2c_nn_dist\": 0.9338405903389017,\n",
    "    \"log_ad\": np.log10(0.31)\n",
    "}\n",
    "\n",
    "observation_ddcp = {\n",
    "    \"slope\": 2.15,\n",
    "    \"log_mean_c2c_nn_dist\": 0.6685,\n",
    "    \"log_ad\": np.log10(0.35)\n",
    "}\n",
    "\n",
    "observation_test1 = {\n",
    "    \"slope\": 2.15,\n",
    "    \"log_mean_c2c_nn_dist\": 0.5585,\n",
    "    \"log_ad\": np.log10(0.50)\n",
    "}\n",
    "\n",
    "# bandwidths_data = [\n",
    "#     # [0.1, 0.002, 0.30],\n",
    "#     # [0.1, 0.001, 0.30],\n",
    "#     # [0.1, 0.0005, 0.30],\n",
    "#     # [0.1, 0.0001, 0.30],\n",
    "    \n",
    "#     # [0.1, 0.005, 0.30],\n",
    "#     # [0.1, 0.0025, 0.30],\n",
    "#     # [0.1, 0.005, 0.15],\n",
    "#     # [0.1, 0.0025, 0.15],\n",
    "\n",
    "#     # [0.1, 0.01, 0.15],\n",
    "#     # [0.1, 0.05, 0.15],\n",
    "#     # [0.1, 0.02, 0.30],\n",
    "\n",
    "#     # This set relatively successful\n",
    "#     # Wide seems to be good\n",
    "#     # [0.15, 0.05, 0.25],\n",
    "#     # [0.15, 0.075, 0.25],\n",
    "#     # [0.15, 0.1, 0.25],\n",
    "\n",
    "#     [0.1, 0.05, 0.2],\n",
    "#     [0.1, 0.1, 0.2],\n",
    "# ]\n",
    "# bandwidths = pd.DataFrame(bandwidths_data, columns=[\"slope\", \"log_mean_c2c_nn_dist\", \"log_ad\"])\n",
    "\n",
    "bandwidths_data = [\n",
    "    [0.1, 0.05, 0.2],\n",
    "]\n",
    "bandwidths = pd.DataFrame(bandwidths_data, columns=[\"slope\", \"log_mean_c2c_nn_dist\", \"log_ad\"])\n",
    "\n",
    "sensitivity_steps = {\n",
    "    \"slope\": 0.1,\n",
    "    \"log_mean_c2c_nn_dist\": 0.05,\n",
    "    \"log_ad\": 0.2\n",
    "}\n",
    "\n",
    "n_steps = 2"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8f2e40b-00d6-4ebf-8696-4562177f8bf3",
   "metadata": {},
   "source": [],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9d28c88-d09e-444b-be0b-730d547b2a8d",
   "metadata": {},
   "source": [],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "39b1bff6-f1bc-4533-ae36-6b43cbaf7cc5",
   "metadata": {},
   "source": [
    "# Dione Region 1: DEE\n",
    "observation = observation_dee\n",
    "\n",
    "test = pd.DataFrame(\n",
    "    map(dict, itertools.product(*[\n",
    "        [\n",
    "            (k, observation[k] + x * sensitivity_steps[k])\n",
    "            for x in range(-n_steps, n_steps + 1)\n",
    "        ]\n",
    "        for k, v in observation.items()\n",
    "    ]))\n",
    ")\n",
    "\n",
    "test = spark.createDataFrame(test).cache()\n",
    "test.count()\n",
    "\n",
    "result = get_confidence_intervals(train, test, predictor_variables, bandwidths, spark)\n",
    "dee_result = result\n",
    "result"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9a7c916b-545c-496e-8a66-899c8e866e1d",
   "metadata": {},
   "source": [
    "result = dee_result\n",
    "observation = observation_dee\n",
    "result.to_parquet(\"data/dee.parquet\")\n",
    "\n",
    "result[\n",
    "    (result.test_slope == observation[\"slope\"])\n",
    "    & (result.test_log_mean_c2c_nn_dist == observation[\"log_mean_c2c_nn_dist\"])\n",
    "    & (result.test_log_ad == observation[\"log_ad\"])\n",
    "]"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "89acbe47-7902-4bf9-934e-54ee27329c63",
   "metadata": {},
   "source": [
    "np.log10(\n",
    "    result[\n",
    "        (result.test_slope == observation[\"slope\"])\n",
    "        & (result.test_log_mean_c2c_nn_dist == observation[\"log_mean_c2c_nn_dist\"])\n",
    "        & (result.test_log_ad == observation[\"log_ad\"])\n",
    "    ].percentiles.iloc[0]\n",
    ")"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7048eaa3-5be9-43b8-a746-9044cbb5e2ec",
   "metadata": {},
   "source": [
    "magnitudes = np.log10(\n",
    "    result[\n",
    "        (result.test_slope == observation[\"slope\"])\n",
    "        & (result.test_log_mean_c2c_nn_dist == observation[\"log_mean_c2c_nn_dist\"])\n",
    "        & (result.test_log_ad == observation[\"log_ad\"])\n",
    "    ].percentiles.iloc[0]\n",
    ")\n",
    "magnitudes[1] - magnitudes[0]"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e469cfb0-7fe6-4301-b23a-54d94b820261",
   "metadata": {},
   "source": [],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "624acbbc-c272-4ed2-8a59-d295bafe04bc",
   "metadata": {},
   "source": [
    "# Dione region 2: DSP\n",
    "observation = observation_dsp\n",
    "\n",
    "test = pd.DataFrame(\n",
    "    map(dict, itertools.product(*[\n",
    "        [\n",
    "            (k, observation[k] + x * sensitivity_steps[k])\n",
    "            for x in range(-n_steps, n_steps + 1)\n",
    "        ]\n",
    "        for k, v in observation.items()\n",
    "    ]))\n",
    ")\n",
    "\n",
    "test = spark.createDataFrame(test).cache()\n",
    "test.count()\n",
    "\n",
    "result = get_confidence_intervals(train, test, predictor_variables, bandwidths, spark)\n",
    "dsp_result = result\n",
    "result"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c05bde7a-5144-4bf1-a055-da40fad3d311",
   "metadata": {},
   "source": [
    "result = dsp_result\n",
    "observation = observation_dsp\n",
    "result.to_parquet(\"data/dsp.parquet\")\n",
    "\n",
    "result[\n",
    "    (result.test_slope == observation[\"slope\"])\n",
    "    & (result.test_log_mean_c2c_nn_dist == observation[\"log_mean_c2c_nn_dist\"])\n",
    "    & (result.test_log_ad == observation[\"log_ad\"])\n",
    "]"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5c61445c-ff1a-4c27-8490-a18392515a5a",
   "metadata": {},
   "source": [
    "np.log10(\n",
    "    result[\n",
    "        (result.test_slope == observation[\"slope\"])\n",
    "        & (result.test_log_mean_c2c_nn_dist == observation[\"log_mean_c2c_nn_dist\"])\n",
    "        & (result.test_log_ad == observation[\"log_ad\"])\n",
    "    ].percentiles.iloc[0]\n",
    ")"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "563fd1f1-ac78-4051-bbaf-ca2e33317645",
   "metadata": {},
   "source": [
    "magnitudes = np.log10(\n",
    "    result[\n",
    "        (result.test_slope == observation[\"slope\"])\n",
    "        & (result.test_log_mean_c2c_nn_dist == observation[\"log_mean_c2c_nn_dist\"])\n",
    "        & (result.test_log_ad == observation[\"log_ad\"])\n",
    "    ].percentiles.iloc[0]\n",
    ")\n",
    "magnitudes[1] - magnitudes[0]"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba290a9b-6112-42f6-8070-83e94ddeb40a",
   "metadata": {},
   "source": [],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b8ac2ed0-4b80-475c-b6f7-d4668f3bab47",
   "metadata": {},
   "source": [
    "# Dione region 3: DICP\n",
    "observation = observation_dicp\n",
    "\n",
    "test = pd.DataFrame(\n",
    "    map(dict, itertools.product(*[\n",
    "        [\n",
    "            (k, observation[k] + x * sensitivity_steps[k])\n",
    "            for x in range(-n_steps, n_steps + 1)\n",
    "        ]\n",
    "        for k, v in observation.items()\n",
    "    ]))\n",
    ")\n",
    "\n",
    "test = spark.createDataFrame(test).cache()\n",
    "test.count()\n",
    "\n",
    "result = get_confidence_intervals(train, test, predictor_variables, bandwidths, spark)\n",
    "dicp_result = result\n",
    "result"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4c1f2096-485d-4837-b931-c8cc358a4c6e",
   "metadata": {},
   "source": [
    "result = dicp_result\n",
    "observation = observation_dicp\n",
    "result.to_parquet(\"data/dicp.parquet\")\n",
    "\n",
    "result[\n",
    "    (result.test_slope == observation[\"slope\"])\n",
    "    & (result.test_log_mean_c2c_nn_dist == observation[\"log_mean_c2c_nn_dist\"])\n",
    "    & (result.test_log_ad == observation[\"log_ad\"])\n",
    "]"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "fb26c4da-91f6-4bf0-a064-14797ad1e326",
   "metadata": {},
   "source": [
    "np.log10(\n",
    "    result[\n",
    "        (result.test_slope == observation[\"slope\"])\n",
    "        & (result.test_log_mean_c2c_nn_dist == observation[\"log_mean_c2c_nn_dist\"])\n",
    "        & (result.test_log_ad == observation[\"log_ad\"])\n",
    "    ].percentiles.iloc[0]\n",
    ")"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4a6018c1-ed56-4224-80cd-56e9883dad25",
   "metadata": {},
   "source": [
    "magnitudes = np.log10(\n",
    "    result[\n",
    "        (result.test_slope == observation[\"slope\"])\n",
    "        & (result.test_log_mean_c2c_nn_dist == observation[\"log_mean_c2c_nn_dist\"])\n",
    "        & (result.test_log_ad == observation[\"log_ad\"])\n",
    "    ].percentiles.iloc[0]\n",
    ")\n",
    "magnitudes[1] - magnitudes[0]"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbafc606-2619-4ea4-9e5b-fb4189af0afb",
   "metadata": {},
   "source": [],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b0c63969-dc1f-4055-86fa-bebbca8a733e",
   "metadata": {},
   "source": [
    "# Dione region 4: DDCP\n",
    "observation = observation_ddcp\n",
    "\n",
    "test = pd.DataFrame(\n",
    "    map(dict, itertools.product(*[\n",
    "        [\n",
    "            (k, observation[k] + x * sensitivity_steps[k])\n",
    "            for x in range(-n_steps, n_steps + 1)\n",
    "        ]\n",
    "        for k, v in observation.items()\n",
    "    ]))\n",
    ")\n",
    "\n",
    "test = spark.createDataFrame(test).cache()\n",
    "test.count()\n",
    "\n",
    "result = get_confidence_intervals(train, test, predictor_variables, bandwidths, spark)\n",
    "ddcp_result = result\n",
    "result"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6d3b6512-84d1-48e3-8068-e57980ea6253",
   "metadata": {},
   "source": [
    "result = ddcp_result\n",
    "observation = observation_ddcp\n",
    "result.to_parquet(\"data/ddcp.parquet\")\n",
    "\n",
    "result[\n",
    "    (result.test_slope == observation[\"slope\"])\n",
    "    & (result.test_log_mean_c2c_nn_dist == observation[\"log_mean_c2c_nn_dist\"])\n",
    "    & (result.test_log_ad == observation[\"log_ad\"])\n",
    "]"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b4811357-6d14-4f76-9bd8-474ee202dd0a",
   "metadata": {},
   "source": [
    "np.log10(\n",
    "    result[\n",
    "        (result.test_slope == observation[\"slope\"])\n",
    "        & (result.test_log_mean_c2c_nn_dist == observation[\"log_mean_c2c_nn_dist\"])\n",
    "        & (result.test_log_ad == observation[\"log_ad\"])\n",
    "    ].percentiles.iloc[0]\n",
    ")"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a50ee8f3-d142-479c-98cf-9d465a9588f2",
   "metadata": {},
   "source": [
    "magnitudes = np.log10(\n",
    "    result[\n",
    "        (result.test_slope == observation[\"slope\"])\n",
    "        & (result.test_log_mean_c2c_nn_dist == observation[\"log_mean_c2c_nn_dist\"])\n",
    "        & (result.test_log_ad == observation[\"log_ad\"])\n",
    "    ].percentiles.iloc[0]\n",
    ")\n",
    "magnitudes[1] - magnitudes[0]"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96751b30-23c2-411f-a902-1a26811031ce",
   "metadata": {},
   "source": [],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "db8c9217-a492-4e67-8524-12ca88b164fc",
   "metadata": {},
   "source": [
    "# Test hypothetical region 1\n",
    "target = \"n_craters_added_in_study_region\"\n",
    "predictor_variables = [\n",
    "    \"slope\",\n",
    "    \"log_mean_c2c_nn_dist\",\n",
    "    # \"log_ad\"\n",
    "]\n",
    "\n",
    "observation_test1 = {\n",
    "    \"slope\": 2.15,\n",
    "    \"log_mean_c2c_nn_dist\": 0.90,\n",
    "    # \"log_ad\": np.log10(0.50)\n",
    "}\n",
    "\n",
    "bandwidths_data = [\n",
    "    [0.1, 0.025],\n",
    "]\n",
    "bandwidths = pd.DataFrame(bandwidths_data, columns=[\"slope\", \"log_mean_c2c_nn_dist\"])\n",
    "\n",
    "sensitivity_steps = {\n",
    "    \"slope\": 0.1,\n",
    "    \"log_mean_c2c_nn_dist\": 0.01,\n",
    "}\n",
    "n_steps = 3\n",
    "\n",
    "\n",
    "observation = observation_test1\n",
    "\n",
    "test = pd.DataFrame(\n",
    "    map(dict, itertools.product(*[\n",
    "        [\n",
    "            (k, observation[k] + x * sensitivity_steps[k])\n",
    "            for x in range(-n_steps, n_steps + 1)\n",
    "        ]\n",
    "        for k, v in observation.items()\n",
    "    ]))\n",
    ")\n",
    "\n",
    "test = spark.createDataFrame(test).cache()\n",
    "test.count()\n",
    "\n",
    "result = get_confidence_intervals(train, test, predictor_variables, bandwidths, spark)\n",
    "test1_result = result\n",
    "result.sort_values([\"test_slope\", \"test_log_mean_c2c_nn_dist\"])"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "e30ad83c-5a4e-41cf-b832-056081329c44",
   "metadata": {},
   "source": [
    "magnitudes = np.log10(\n",
    "    result[\n",
    "        (result.test_slope == observation[\"slope\"])\n",
    "        & (result.test_log_mean_c2c_nn_dist == observation[\"log_mean_c2c_nn_dist\"])\n",
    "        # & (result.test_log_ad == observation[\"log_ad\"])\n",
    "    ].percentiles.iloc[0]\n",
    ")\n",
    "magnitudes[1] - magnitudes[0]"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b0902af-2a38-456a-8f7a-0b0c4d0858a3",
   "metadata": {},
   "source": [],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dedc9a3-453b-41ee-92ad-f740a69c65d8",
   "metadata": {},
   "source": [],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "621a56c1-48c8-407f-b678-1eb6ef60eea4",
   "metadata": {},
   "source": [
    "# Reload results\n",
    "dee_result = pd.read_parquet(\"data/dee.parquet\")\n",
    "ddcp_result = pd.read_parquet(\"data/ddcp.parquet\")\n",
    "dsp_result = pd.read_parquet(\"data/dsp.parquet\")\n",
    "dicp_result = pd.read_parquet(\"data/dicp.parquet\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f537566-71e9-443c-aa62-5f718fdfaed6",
   "metadata": {},
   "source": [
    "dee_result[\"percentiles\"] = [[int(y) for y in x.strip(\"[]\").split(\", \")] if type(x) != float else None for x in dee_result.percentiles]\n",
    "ddcp_result[\"percentiles\"] = [[int(y) for y in x.strip(\"[]\").split(\", \")] if type(x) != float else None for x in ddcp_result.percentiles]\n",
    "dsp_result[\"percentiles\"] = [[int(y) for y in x.strip(\"[]\").split(\", \")] if type(x) != float else None for x in dsp_result.percentiles]\n",
    "dicp_result[\"percentiles\"] = [[int(y) for y in x.strip(\"[]\").split(\", \")] if type(x) != float else None for x in dicp_result.percentiles]"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6c1d122-0f41-4b33-8d67-403308a726e7",
   "metadata": {},
   "source": [
    "# Plots of orders of magnitude\n",
    "observation = observation_dee\n",
    "result = dee_result\n",
    "result[\"orders_of_magnitude\"] = [np.log10(x[1] / x[0]) if x else None for x in result.loc[:, \"percentiles\"]] \n",
    "\n",
    "r = result[\n",
    "    (result.bandwidth_log_mean_c2c_nn_dist == 0.1)\n",
    "    & (~result.orders_of_magnitude.isna())\n",
    "].copy()\n",
    "fig = px.scatter_3d(\n",
    "    r,\n",
    "    x=\"test_slope\",\n",
    "    y=\"test_log_mean_c2c_nn_dist\",\n",
    "    z=\"test_log_ad\",\n",
    "    color=\"orders_of_magnitude\",\n",
    "    size_max=1,\n",
    "    width=1000,\n",
    "    height=800\n",
    ")\n",
    "fig.update_traces(marker={\"size\":7})\n",
    "fig.show()"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fb6fff9-8feb-4452-8455-8ce615e46f18",
   "metadata": {},
   "source": [],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "783de993-de33-4821-a5aa-5b4762f5454a",
   "metadata": {},
   "source": [
    "observation = observation_dsp\n",
    "result = dsp_result\n",
    "result[\"orders_of_magnitude\"] = [np.log10(x[1] / x[0]) if x else None for x in result.loc[:, \"percentiles\"]] "
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3e3fde3-3fbe-4b71-a26f-d57f6d885b85",
   "metadata": {},
   "source": [
    "result[\n",
    "    #(result.bandwidth_log_mean_c2c_nn_dist == 0.1)\n",
    "    (result.test_slope == observation[\"slope\"])\n",
    "    & (result.test_log_mean_c2c_nn_dist == observation[\"log_mean_c2c_nn_dist\"])\n",
    "    & (result.test_log_ad == observation[\"log_ad\"])\n",
    "]"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "630a6d9d-01ac-4f64-bc76-b76663a3e5f2",
   "metadata": {},
   "source": [],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a7b1098-ef7f-4112-840a-acd1816c8db6",
   "metadata": {},
   "source": [],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "49910bce-3926-42e3-b725-7e519867e8fd",
   "metadata": {},
   "source": [
    "## Scoring the selected parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3968866b-99fd-45f4-81e4-7800d825d48c",
   "metadata": {},
   "source": [
    "train = spark.read.parquet(\"data/train_0.250_10000.parquet\")\n",
    "test = spark.read.parquet(\"data/test_0.250_10000.parquet\").limit(5000).cache()\n",
    "test.count()"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e31b17a0-967f-4685-b4b5-1affd716f485",
   "metadata": {},
   "source": [
    "target = \"n_craters_added_in_study_region\"\n",
    "predictor_variables = [\n",
    "    \"slope\",\n",
    "    \"log_mean_c2c_nn_dist\",\n",
    "    \"log_ad\"\n",
    "]\n",
    "\n",
    "bandwidths_data = [\n",
    "    [0.1, 0.1, 0.2],\n",
    "]\n",
    "bandwidths = pd.DataFrame(bandwidths_data, columns=[\"slope\", \"log_mean_c2c_nn_dist\", \"log_ad\"])\n",
    "\n",
    "result = get_confidence_intervals(train, test, predictor_variables, bandwidths, spark)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6634f1a6-6c4b-40b9-b9f7-e70568babfed",
   "metadata": {},
   "source": [
    "# result = pd.read_csv(\"data/scored.csv\")\n",
    "# # result[\"percentiles\"] = [[int(y) for y in x.strip(\"[]\").split(\", \")] if type(x) != float else None for x in result.percentiles]\n",
    "# result"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "260860fb-f02c-49ac-b5cb-7734cfc00409",
   "metadata": {},
   "source": [
    "# result.to_csv(\"data/scored.csv\")\n",
    "result.to_parquet(\"data/scored.parquet\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a8f338b-0443-4829-aff2-f7186b043dc2",
   "metadata": {},
   "source": [
    "result = pd.read_parquet(\"data/scored.parquet\")\n",
    "result"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "966dda45-1ab0-428e-8cd1-3a1a89c416e3",
   "metadata": {},
   "source": [
    "result[\"orders_of_magnitude\"] = [np.log10(x[1] / x[0]) if x else None for x in result.loc[:, \"percentiles\"]] "
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a4761f1-2ef2-42fc-9180-cf51c4efe59e",
   "metadata": {},
   "source": [
    "plt.hist(result.target, bins=50)\n",
    "plt.show()"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6fd05fe-1b98-42f2-a824-f7b50d420e48",
   "metadata": {},
   "source": [
    "ci_high = result.percentiles.map(lambda x: x[-1])\n",
    "ci_low = result.percentiles.map(lambda x: x[0])\n",
    "inside_ci = ((result.target >= ci_low) & (result.target <= ci_high))\n",
    "percent_inside_ci = inside_ci.mean()"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "795a8712-2d44-4701-932b-049b02b57ee0",
   "metadata": {},
   "source": [
    "plt.scatter(\n",
    "    \n",
    ")"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21c5cbf5-4055-4ccd-ab46-f944041ee4fb",
   "metadata": {},
   "source": [],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f5b7a47-4837-4b60-bd16-19effa4e05d1",
   "metadata": {},
   "source": [],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "44079c69-0878-4928-93cc-f5677aeb8dbb",
   "metadata": {},
   "source": [
    "## Evaluating the model by ranges of N"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1028fd7-b739-4193-bdc5-ad3f06f6e8a5",
   "metadata": {},
   "source": [
    "train = spark.read.parquet(f\"data/train_1.000_{int(1e9)}.parquet\")\n",
    "test = spark.read.parquet(f\"data/test_1.000_{int(1e9)}.parquet\").cache()\n",
    "test.count()"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87270041-1eaa-4fd3-93bb-32ec9fb8f730",
   "metadata": {},
   "source": [
    "target = \"n_craters_added_in_study_region\"\n",
    "predictor_variables = [\n",
    "    \"slope\",\n",
    "    \"log_mean_c2c_nn_dist\",\n",
    "    \"log_ad\"\n",
    "]\n",
    "\n",
    "log_n_start = np.log10(100)\n",
    "log_n_end = np.log10(200000)\n",
    "n_steps = 25\n",
    "n_tests_per_step = 1000\n",
    "step_size = (log_n_end - log_n_start) / (n_steps - 1)\n",
    "\n",
    "\n",
    "# bandwidths_data = [\n",
    "#     [0.05, 0.01, 0.2],\n",
    "# ]\n",
    "\n",
    "# Result: CI's cover way more than 95%\n",
    "# bandwidths_data = [\n",
    "#     [0.1, 0.05, 0.2],\n",
    "# ]\n",
    "bandwidths = pd.DataFrame(bandwidths_data, columns=[\"slope\", \"log_mean_c2c_nn_dist\", \"log_ad\"])\n",
    "\n",
    "results = dict()\n",
    "for step in range(n_steps):\n",
    "    print(f\"Step {step}...\")\n",
    "\n",
    "    log_n_target = log_n_start + step_size * step\n",
    "    test_subset = test.where(\n",
    "        test.target.between(10**(log_n_target - step_size / 2), 10**(log_n_target + step_size / 2))\n",
    "    ).orderBy(F.rand()).limit(n_tests_per_step).cache()\n",
    "    test_subset.count()\n",
    "\n",
    "    r = get_confidence_intervals(train, test_subset, predictor_variables, bandwidths, spark)\n",
    "    r[\"orders_of_magnitude\"] = [np.log10(x[1] / x[0]) if x else None for x in r.loc[:, \"percentiles\"]]\n",
    "    ci_high = r.percentiles.map(lambda x: x[-1] if x else None)\n",
    "    ci_low = r.percentiles.map(lambda x: x[0] if x else None)\n",
    "    r[\"inside_ci\"] = ((r.target >= ci_low) & (r.target <= ci_high))\n",
    "    \n",
    "    print(f\"% inside CI: {r.inside_ci.mean()}\")\n",
    "    print(f\"Mean orders of magnitude: {r.orders_of_magnitude.mean()}\")\n",
    "    \n",
    "    results[step] = r"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef3f0462-c5cf-47bc-b6cf-1205eae7cf0e",
   "metadata": {},
   "source": [
    "# old_results = results\n",
    "# old_results_90_big = results\n",
    "# results_only_two_vars = results"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5818e18-f73c-4ef5-b9cd-3602eeff5164",
   "metadata": {},
   "source": [
    "results = old_results"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "535ac963-a0c9-4313-9e63-f7b308d0b70e",
   "metadata": {},
   "source": [
    "results[0]"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea588a83-18f7-48d1-b97a-f2ce914e1f6b",
   "metadata": {},
   "source": [
    "all = pd.concat([x for x in results.values()], axis=0)\n",
    "all.inside_ci.mean()"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "461ee7b7-5d27-4bf4-bcd0-c66a1451692f",
   "metadata": {},
   "source": [
    "all.orders_of_magnitude.mean()"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b067c399-c885-4355-a5c5-6b359b2a3013",
   "metadata": {},
   "source": [
    "[results[x].inside_ci.mean() for x in range(len(results))]"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79065e58-f8dc-4d22-89e6-bc5d9bdf5863",
   "metadata": {},
   "source": [
    "n_steps = 25\n",
    "plt.scatter(\n",
    "    [log_n_start + x * step_size for x in range(n_steps)],\n",
    "    [results[x].inside_ci.mean() for x in range(n_steps)],\n",
    ")"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "816b7e2d-162e-494a-90b8-fb7ef82e60eb",
   "metadata": {},
   "source": [
    "[results[x].orders_of_magnitude.mean() for x in range(len(results))]"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "844cbf86-06dc-4a23-900f-24a154ca3516",
   "metadata": {},
   "source": [],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "e699775f-6d37-4185-89fe-711678669eb2",
   "metadata": {},
   "source": [
    "### Visualization of log_N vs NN_d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b04397ba-e321-46bd-bef3-9aa2d24a3b7a",
   "metadata": {},
   "source": [
    "def join_configs(data: DataFrame, configs: DataFrame) -> DataFrame:\n",
    "    data.createOrReplaceTempView(\"data\")\n",
    "    configs.createOrReplaceTempView(\"configs\")\n",
    "    \n",
    "    # Join data and config\n",
    "    query = f\"\"\"\n",
    "    SELECT\n",
    "        *\n",
    "    FROM\n",
    "        data\n",
    "        INNER JOIN configs\n",
    "            ON data.simulation_id = configs.simulation_id\n",
    "    \"\"\"\n",
    "    return spark.sql(query)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c8e1ec0-d0d8-4921-b3a7-01e2d6bc2075",
   "metadata": {},
   "source": [
    "data_subset = data.where(\n",
    "    (F.col(\"information_remaining\") > F.lit(0.25))\n",
    "    & (F.col(\"n_craters_added_in_study_region\") > F.lit(50))\n",
    ").sample(0.00025)\n",
    "\n",
    "df = join_configs(data_subset, configs_df).toPandas()"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfbcaed6-5682-4260-9f02-ca54a52694f9",
   "metadata": {},
   "source": [
    "df[\"log_center_to_center_nearest_neighbor_distance_mean\"] = np.log10(df.center_to_center_nearest_neighbor_distance_mean)\n",
    "df[\"log_n_craters_added_in_study_region\"] = np.log10(df.n_craters_added_in_study_region)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9708761-fec6-4816-9cb7-578ced87e003",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "fig = px.scatter(\n",
    "    df,\n",
    "    x=\"log_mean_c2c_nn_dist\",\n",
    "    y=\"log_n_craters_added_in_study_region\",\n",
    "    color=\"slope\",\n",
    "    hover_data=[\"slope\", \"n_craters_added_in_study_region\", \"areal_density\", \"n_craters_in_study_region\"],\n",
    "    size_max=1,\n",
    "    width=1600,\n",
    "    height=600,\n",
    ")\n",
    "fig.update_layout(\n",
    "    xaxis_title=dict(\n",
    "        text=\"$log_{10}(\\overline{NN_d})$\",\n",
    "        font=dict(size=18)\n",
    "    ),\n",
    "    yaxis_title=dict(\n",
    "        text=\"$log_{10}(N_{tot})$\",\n",
    "        font=dict(size=18)\n",
    "    ),\n",
    ")\n",
    "fig.update_traces(marker={\"size\":3})\n",
    "fig.show()"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c469037a-1c9a-413f-9697-c3ea15305b64",
   "metadata": {},
   "source": [],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e290da5-880c-4c93-9cdf-1bbaefb0b60e",
   "metadata": {},
   "source": [
    "# Trying a range selector\n",
    "to_show = df.copy()\n",
    "to_show[\"slope_selector\"] = ((to_show.slope - 1) * 10).astype(\"int\") / 10 + 1\n",
    "to_show = to_show.sort_values(\"slope_selector\")\n",
    "to_show[\"index\"] = range(to_show.shape[0])\n",
    "\n",
    "range_x = [to_show.log_mean_c2c_nn_dist.min(), to_show.log_mean_c2c_nn_dist.max()]\n",
    "range_y = [to_show.log_n_craters_added_in_study_region.min(), to_show.log_n_craters_added_in_study_region.max()]\n",
    "\n",
    "fig = px.scatter(\n",
    "    to_show,\n",
    "    x=\"log_mean_c2c_nn_dist\",\n",
    "    y=\"log_n_craters_added_in_study_region\",\n",
    "    hover_data=[\"slope\", \"n_craters_added_in_study_region\"],\n",
    "    size_max=1,\n",
    "    animation_frame=\"slope_selector\",\n",
    "    animation_group=\"index\",\n",
    "    range_x=range_x,\n",
    "    range_y=range_y,\n",
    "    width=1000,\n",
    "    height=500\n",
    ")\n",
    "fig.update_layout(\n",
    "    xaxis_title=dict(\n",
    "        text=\"$log_{10}(\\overline{NN_d})$\",\n",
    "        font=dict(size=18)\n",
    "    ),\n",
    "    yaxis_title=dict(\n",
    "        text=\"$log(N_{tot})$\",\n",
    "        font=dict(size=18)\n",
    "    ),\n",
    ")\n",
    "fig.update_traces(marker={\"size\":3})\n",
    "fig.show()"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b16eaeb-0198-4193-83a2-97a18ee37b70",
   "metadata": {},
   "source": [],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e394d7d-987f-4f1d-848d-63081ac5e030",
   "metadata": {},
   "source": [],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d2139c3-b1eb-4012-b28d-1953d5d7da34",
   "metadata": {},
   "source": [],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1a20d2a-584d-4617-bc17-b81a4ff7617a",
   "metadata": {},
   "source": [],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "c6345465-9e79-48c8-bc5c-c8b9e7f7497c",
   "metadata": {},
   "source": [
    "## Plotting single simulations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f679cf6-0fc1-4622-b867-c8645c5c4665",
   "metadata": {},
   "source": [
    "cdf = configs_df.toPandas()"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "055546fa-78c6-48d5-b484-80a4adf06074",
   "metadata": {},
   "source": [
    "cdf"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d7f9b0e-49fc-42e1-a250-8c0a19e9d0f7",
   "metadata": {},
   "source": [
    "df = data.where(\n",
    "    (F.col(\"information_remaining\") > F.lit(0.05))\n",
    "    & (F.col(\"n_craters_added_in_study_region\") > F.lit(50))\n",
    "    & (df.simulation_id == 1742)\n",
    "    # & (df.simulation_id == 7230)\n",
    ")"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7da03e91-3ffb-47d0-b968-ffb0084a9fe7",
   "metadata": {},
   "source": [
    "n_pts = 10000\n",
    "n_obs = df.count()\n",
    "\n",
    "if n_obs > n_pts:\n",
    "    sample_fraction = n_pts / n_obs\n",
    "    df = df.sample(sample_fraction)\n",
    "\n",
    "pandas_df = df.toPandas()"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6bf47f8-b1f9-4c52-b9f9-176ab115f61e",
   "metadata": {},
   "source": [
    "pandas_df"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e8fd6ec-5996-44f3-a980-9bfb61646e6b",
   "metadata": {},
   "source": [
    "plt.figure(figsize=(10, 4))\n",
    "plt.plot(\n",
    "    np.log10(pandas_df.n_craters_added_in_study_region),\n",
    "    pandas_df.log_mean_c2c_nn_dist,    \n",
    ")\n",
    "plt.title(\"Single Simulation, b=1.65\")\n",
    "plt.xlabel(\"$log_{10}(N_{tot})$\", size=14)\n",
    "plt.ylabel(\"$log_{10}(\\overline{NN_d})$\", size=14)\n",
    "plt.show()"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab2c7001-52f0-4745-9622-37a15e8b354a",
   "metadata": {},
   "source": [],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab10ec6a-a240-4fcd-bb35-789268e867d8",
   "metadata": {},
   "source": [
    "n_pts = 10000\n",
    "\n",
    "simulation_ids = list(cdf[cdf.slope.between(1.66, 1.68)].sample(4).simulation_id)\n",
    "simulation_ids = [1742] + simulation_ids\n",
    "\n",
    "plt.figure(figsize=(10, 4))\n",
    "\n",
    "for idx, simulation_id in enumerate(simulation_ids):\n",
    "    df = data.where(\n",
    "        (F.col(\"information_remaining\") > F.lit(0.05))\n",
    "        & (F.col(\"n_craters_added_in_study_region\") > F.lit(50))\n",
    "        & (df.simulation_id == simulation_id)\n",
    "    )\n",
    "\n",
    "    n_obs = df.count()\n",
    "    sample_fraction = n_pts / n_obs\n",
    "    df = df.sample(sample_fraction)\n",
    "    pandas_df = df.toPandas()\n",
    "    pandas_df = pandas_df.sort_values(\"n_craters_added_in_study_region\")\n",
    "    \n",
    "    plt.plot(\n",
    "        np.log10(pandas_df.n_craters_added_in_study_region),\n",
    "        pandas_df.log_mean_c2c_nn_dist,\n",
    "        label = f\"Sim {idx + 1}\"\n",
    "    )\n",
    "\n",
    "plt.title(\"Simulations With $b \\in (1.65, 1.70)$\")\n",
    "plt.xlabel(\"$log_{10}(N_{tot})$\", size=14)\n",
    "plt.ylabel(\"$log_{10}(\\overline{NN_d})$\", size=14)\n",
    "plt.legend()\n",
    "plt.show()"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4607751-012c-4ec5-9f95-fb6184f9bac4",
   "metadata": {},
   "source": [],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3858c8a8-9355-452d-933f-3d2a2c0f5665",
   "metadata": {},
   "source": [],
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
