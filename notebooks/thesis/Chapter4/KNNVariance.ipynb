{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e01ce1bc-1aa7-4abe-bd1d-32f117725479",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import yaml\n",
    "from typing import *\n",
    "\n",
    "import pyspark\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql import SparkSession, DataFrame, Window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6f2311e7-f80a-4d67-afb7-7b284fdffbb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/10/22 17:13:40 WARN Utils: Your hostname, muninn resolves to a loopback address: 127.0.1.1; using 192.168.86.20 instead (on interface enp8s0)\n",
      "23/10/22 17:13:40 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "23/10/22 17:13:40 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "23/10/22 17:13:41 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n",
      "23/10/22 17:13:41 WARN Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.\n"
     ]
    }
   ],
   "source": [
    "n_cores = 30\n",
    "\n",
    "spark = (SparkSession.builder\n",
    "         .master(f\"local[{n_cores}]\")\n",
    "         .appName(\"Saturation\")\n",
    "         .config(\"spark.sql.shuffle.partitions\", \"1000\")\n",
    "         .config(\"spark.driver.memory\", \"64g\")\n",
    "         .config(\"spark.driver.maxResultSize\", \"8g\")\n",
    "         .getOrCreate())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2198839d-a709-4d91-bffc-fad7122f7235",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_config(path: Path) -> Dict:\n",
    "    with path.open(\"r\") as config_file:\n",
    "        config = yaml.safe_load(config_file)\n",
    "    return config\n",
    "\n",
    "\n",
    "def read_configs(base_path: str, spark_session: SparkSession) -> pyspark.RDD:\n",
    "    completed_filenames = list(Path(base_path).glob(\"*/completed.txt\"))\n",
    "    configs = map(lambda x: x.parent / \"config.yaml\", completed_filenames)\n",
    "    configs = map(read_config, configs)\n",
    "    return spark_session.sparkContext.parallelize(configs)\n",
    "\n",
    "\n",
    "def create_configs_df(configs: pyspark.RDD) -> DataFrame:\n",
    "    config_columns = [\n",
    "        \"simulation_id\",\n",
    "        \"slope\",\n",
    "        \"r_stat_multiplier\",\n",
    "        \"effective_radius_multiplier\",\n",
    "        \"min_rim_percentage\"\n",
    "    ]\n",
    "    return configs.map(lambda x: {k: v for k, v in x.items() if k in config_columns}).toDF()\n",
    "\n",
    "def add_post_saturation_percentiles(data: DataFrame, column: str):\n",
    "    \"\"\"\n",
    "    Calculates the post-saturation percentile of a given column.\n",
    "    \"\"\"\n",
    "    col_dtype = dict(data.dtypes)[column]\n",
    "\n",
    "    # Select all points post-saturation - last 1/3 of each simulation\n",
    "    window = Window.partitionBy(\"simulation_id\").orderBy(F.col(\"n_craters_added_in_study_region\"))\n",
    "    with_row_number = data.withColumn(\"row_number\", F.row_number().over(window))\n",
    "\n",
    "    saturation_points = with_row_number.groupby(\"simulation_id\").agg(F.max(\"row_number\").alias(\"n_rows\"))\n",
    "    saturation_points = saturation_points.withColumn(\"saturation_point\", (F.col(\"n_rows\") / 3 * 2).cast(\"int\"))\n",
    "\n",
    "    with_saturation_points = with_row_number.join(saturation_points, on=\"simulation_id\", how=\"inner\")\n",
    "    post_saturation = (\n",
    "        with_saturation_points\n",
    "        .filter(F.col(\"row_number\") - F.col(\"saturation_point\") >= 0)\n",
    "        .drop(\"row_number\")\n",
    "        .drop(\"saturation_point\")\n",
    "        .drop(\"n_rows\")\n",
    "    )\n",
    "\n",
    "    # Calculate post-saturation percentiles for each simulation\n",
    "    # Create a \"lookup table\" of percentiles by simulation to join to\n",
    "    percentile_lookup = (\n",
    "        post_saturation\n",
    "        .groupby(\"simulation_id\")\n",
    "        .agg(\n",
    "            F.percentile_approx(column, F.array(*[F.lit(x / 100.0) for x in range(1, 100)]), 10000).alias(\"percentiles\")\n",
    "        )\n",
    "        .select(\n",
    "            \"simulation_id\",\n",
    "            F.explode(\n",
    "                F.arrays_zip(\n",
    "                    F.array(*[F.lit(x / 100) for x in range(0, 100)]),\n",
    "                    F.array_insert(\"percentiles\", 1, F.lit(-2 ** 33).cast(col_dtype)),\n",
    "                    F.array_insert(\"percentiles\", 100, F.lit(2 ** 33).cast(col_dtype)),\n",
    "                )\n",
    "            ).alias(\"percentile_array\")\n",
    "        )\n",
    "        .select(\n",
    "            \"simulation_id\",\n",
    "            F.col(\"percentile_array\")[\"0\"].alias(f\"post_saturation_{column}_percentile\"),\n",
    "            F.col(\"percentile_array\")[\"1\"].alias(\"lower\"),\n",
    "            F.col(\"percentile_array\")[\"2\"].alias(\"upper\"),\n",
    "        )\n",
    "    ).cache()\n",
    "\n",
    "    # Join back to the full dataframe to add percentiles to each observation\n",
    "    result = (\n",
    "        data\n",
    "        .join(percentile_lookup, on=\"simulation_id\")\n",
    "        .filter(data[column] >= percentile_lookup.lower)\n",
    "        .filter(data[column] < percentile_lookup.upper)\n",
    "        .drop(\"lower\", \"upper\")\n",
    "    )\n",
    "\n",
    "    return result\n",
    "\n",
    "\n",
    "def quantize_value(value: float, delta: float) -> float:\n",
    "    return np.round(int(np.round(value / delta, decimals=5)) * delta, decimals=5)\n",
    "\n",
    "\n",
    "def get_min_max_n_buckets(column: str, delta: float, df) -> Tuple[float, float, int]:\n",
    "    min_max_df = df.select(F.min(column), F.max(column)).toPandas()\n",
    "    \n",
    "    min_val = quantize_value(min_max_df.iloc[0, 0], delta)\n",
    "    max_val = quantize_value(min_max_df.iloc[0, 1], delta)\n",
    "    n_buckets = int(np.round((max_val - min_val) / delta, decimals=5)) + 1\n",
    "\n",
    "    return min_val, max_val, n_buckets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a7c5bd67-e044-423a-8f40-cde0c804d5f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_path = \"/data/saturation/n_craters_stop_condition_20230918\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7a590e14-bba1-4068-866a-0fb2373ea6cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "r_stat = 5\n",
    "\n",
    "configs_df = create_configs_df(read_configs(base_path, spark))\n",
    "data = spark.read.parquet(f\"{base_path}/*/statistics_*.parquet\")\n",
    "data_with_configs = data.join(F.broadcast(configs_df), on=\"simulation_id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "fd94c21d-ae05-4916-b6c6-2777045b9033",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Predictor variables\n",
    "features = [\n",
    "    \"log_mean_c2c_nn_dist\",\n",
    "    \"slope\"\n",
    "]\n",
    "additional_variables = [\n",
    "    \"simulation_id\",\n",
    "    \"crater_id\",\n",
    "    \"n_craters_added_in_study_region\"\n",
    "]\n",
    "\n",
    "# Add log mean c2c nn distance\n",
    "data_with_features = data_with_configs.select(\"*\",\n",
    "                                              F.log10(F.col(\"center_to_center_nearest_neighbor_distance_mean\") / F.lit(r_stat)).alias(\"log_mean_c2c_nn_dist\"))\n",
    "data_with_features = data_with_features.select(*(features + additional_variables))\n",
    "\n",
    "# Get means/standard deviations in order to standardize features\n",
    "means = [F.mean(x).alias(f\"{x}_mean\") for x in features]\n",
    "stdevs = [F.stddev_samp(x).alias(f\"{x}_stdev\") for x in features]\n",
    "mean_and_stdevs_df = data_with_features.select(*(means + stdevs)).toPandas()\n",
    "mean_and_stdevs_dict = mean_and_stdevs_df.iloc[0].to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "49b688fa-df56-4df9-b439-7c7876be9f3c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['log_mean_c2c_nn_dist',\n",
       " 'slope',\n",
       " 'simulation_id',\n",
       " 'crater_id',\n",
       " 'n_craters_added_in_study_region']"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_with_features.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "06a5b92d-0476-449e-b286-44b9b4d06555",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardize predictor variables\n",
    "data_with_standardized_features = data_with_features\n",
    "for features in features:\n",
    "    data_with_standardized_features = data_with_standardized_features.withColumn(\n",
    "        feature,\n",
    "        (F.col(feature) - F.lit(mean_and_stdevs_dict[f\"{feature}_mean\"])) / F.lit(mean_and_stdevs_dict[f\"{feature}_stdev\"])\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "dff8647a-db7e-4f41-917b-9483681e90c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the nearest neighbors to a specified feature vector\n",
    "feature_vector = {\n",
    "    \"log_mean_c2c_nn_dist\": 1.8,\n",
    "    \"slope\": 2.2\n",
    "}\n",
    "\n",
    "# Standardize the feature vector\n",
    "feature_vector_standardized = {\n",
    "    k: (v - mean_and_stdevs_dict[f\"{k}_mean\"]) / mean_and_stdevs_dict[f\"{k}_stdev\"]\n",
    "    for k, v in feature_vector.items()\n",
    "}\n",
    "\n",
    "# Filter to values within 0.5 * stdev for performance reasons\n",
    "filtered = data_with_standardized_features.withColumn(\"distance\", F.lit(0.0))\n",
    "\n",
    "sum_col = F.lit(\"distance\")\n",
    "for feature, value in feature_vector_standardized.items():\n",
    "    filtered = filtered.where(F.abs(F.lit(value) - F.col(feature)) < 1)\n",
    "    sum_col += F.pow(F.col(feature) - F.lit(value), 2)\n",
    "\n",
    "filtered = filtered.withColumn(\"distance\", sum_col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "2764ab3f-4dc7-47dd-8d02-98b90e316961",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "n_nearest_neighbors = 1000\n",
    "nearest_neighbors = filtered.sort(F.col(\"distance\").desc()).limit(n_nearest_neighbors).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "65c03445-77f3-45bc-baa9-cab1e52cdae4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>log_mean_c2c_nn_dist</th>\n",
       "      <th>slope</th>\n",
       "      <th>simulation_id</th>\n",
       "      <th>crater_id</th>\n",
       "      <th>n_craters_added_in_study_region</th>\n",
       "      <th>distance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5.174134</td>\n",
       "      <td>1.342782</td>\n",
       "      <td>8789</td>\n",
       "      <td>200</td>\n",
       "      <td>11</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5.145896</td>\n",
       "      <td>1.342782</td>\n",
       "      <td>8789</td>\n",
       "      <td>234</td>\n",
       "      <td>12</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4.929405</td>\n",
       "      <td>1.342782</td>\n",
       "      <td>8789</td>\n",
       "      <td>249</td>\n",
       "      <td>13</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.889420</td>\n",
       "      <td>1.342782</td>\n",
       "      <td>8789</td>\n",
       "      <td>259</td>\n",
       "      <td>14</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4.726162</td>\n",
       "      <td>1.342782</td>\n",
       "      <td>8789</td>\n",
       "      <td>267</td>\n",
       "      <td>15</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>995</th>\n",
       "      <td>4.321786</td>\n",
       "      <td>1.337579</td>\n",
       "      <td>1218</td>\n",
       "      <td>261</td>\n",
       "      <td>37</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>996</th>\n",
       "      <td>4.316844</td>\n",
       "      <td>1.337579</td>\n",
       "      <td>1218</td>\n",
       "      <td>263</td>\n",
       "      <td>38</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>997</th>\n",
       "      <td>4.314902</td>\n",
       "      <td>1.337579</td>\n",
       "      <td>1218</td>\n",
       "      <td>269</td>\n",
       "      <td>39</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>998</th>\n",
       "      <td>4.274624</td>\n",
       "      <td>1.337579</td>\n",
       "      <td>1218</td>\n",
       "      <td>277</td>\n",
       "      <td>40</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999</th>\n",
       "      <td>4.279180</td>\n",
       "      <td>1.337579</td>\n",
       "      <td>1218</td>\n",
       "      <td>278</td>\n",
       "      <td>41</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1000 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     log_mean_c2c_nn_dist     slope  simulation_id  crater_id  \\\n",
       "0                5.174134  1.342782           8789        200   \n",
       "1                5.145896  1.342782           8789        234   \n",
       "2                4.929405  1.342782           8789        249   \n",
       "3                4.889420  1.342782           8789        259   \n",
       "4                4.726162  1.342782           8789        267   \n",
       "..                    ...       ...            ...        ...   \n",
       "995              4.321786  1.337579           1218        261   \n",
       "996              4.316844  1.337579           1218        263   \n",
       "997              4.314902  1.337579           1218        269   \n",
       "998              4.274624  1.337579           1218        277   \n",
       "999              4.279180  1.337579           1218        278   \n",
       "\n",
       "     n_craters_added_in_study_region  distance  \n",
       "0                                 11       NaN  \n",
       "1                                 12       NaN  \n",
       "2                                 13       NaN  \n",
       "3                                 14       NaN  \n",
       "4                                 15       NaN  \n",
       "..                               ...       ...  \n",
       "995                               37       NaN  \n",
       "996                               38       NaN  \n",
       "997                               39       NaN  \n",
       "998                               40       NaN  \n",
       "999                               41       NaN  \n",
       "\n",
       "[1000 rows x 6 columns]"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "----------------------------------------\n",
      "Exception occurred during processing of request from ('127.0.0.1', 35792)\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.11/socketserver.py\", line 317, in _handle_request_noblock\n",
      "    self.process_request(request, client_address)\n",
      "  File \"/usr/lib/python3.11/socketserver.py\", line 348, in process_request\n",
      "    self.finish_request(request, client_address)\n",
      "  File \"/usr/lib/python3.11/socketserver.py\", line 361, in finish_request\n",
      "    self.RequestHandlerClass(request, client_address, self)\n",
      "  File \"/usr/lib/python3.11/socketserver.py\", line 755, in __init__\n",
      "    self.handle()\n",
      "  File \"/home/mason/code/saturation/venv_311/lib/python3.11/site-packages/pyspark/accumulators.py\", line 281, in handle\n",
      "    poll(accum_updates)\n",
      "  File \"/home/mason/code/saturation/venv_311/lib/python3.11/site-packages/pyspark/accumulators.py\", line 253, in poll\n",
      "    if func():\n",
      "       ^^^^^^\n",
      "  File \"/home/mason/code/saturation/venv_311/lib/python3.11/site-packages/pyspark/accumulators.py\", line 257, in accum_updates\n",
      "    num_updates = read_int(self.rfile)\n",
      "                  ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/mason/code/saturation/venv_311/lib/python3.11/site-packages/pyspark/serializers.py\", line 596, in read_int\n",
      "    raise EOFError\n",
      "EOFError\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "nearest_neighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "b95887ac-ac03-49b6-bbdd-8a73099a312d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Column<'((distance + POWER((log_mean_c2c_nn_dist - 4.3369333798880945), 2)) + POWER((slope - 0.35396230932620254), 2))'>"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum_col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcbfa431-1af5-4605-b32d-f7672c885a6d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
