{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "34fb5b29-5fdb-4d6c-bc8c-9728b34e51c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import glob\n",
    "import yaml\n",
    "from typing import *\n",
    "from functools import reduce\n",
    "\n",
    "import pyspark\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql import SparkSession, DataFrame, Row, Window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b36eff61-ec11-42d5-ada4-3c9c7c3586fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_path = \"/data/saturation/random_runs_20230812/\"\n",
    "\n",
    "n_cores = 28"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "628fdb7b-c860-4871-98c6-aefdac2cb9fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/08/20 16:58:40 WARN Utils: Your hostname, muninn resolves to a loopback address: 127.0.1.1; using 192.168.86.20 instead (on interface enp8s0)\n",
      "23/08/20 16:58:40 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "23/08/20 16:58:41 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder \\\n",
    "                    .master(f\"local[{n_cores}]\") \\\n",
    "                    .appName(\"Saturation\") \\\n",
    "                    .config(\"spark.driver.memory\", \"64g\") \\\n",
    "                    .getOrCreate()\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f125784-7250-461c-b364-c42798271e11",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "afb4a61e-9af1-40fc-8aed-255bac0c73cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare samples from each simulation, overall and post-saturation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b48108ca-f791-477f-9492-8c85363839c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_config(path: Path) -> Dict:\n",
    "    with path.open(\"r\") as config_file:\n",
    "        config = yaml.safe_load(config_file)\n",
    "    return config\n",
    "\n",
    "def read_configs(base_path: str) -> pyspark.RDD:\n",
    "    completed_filenames = list(Path(base_path).glob(\"*/completed.txt\"))\n",
    "    configs = map(lambda x: x.parent / \"config.yaml\", completed_filenames)\n",
    "    configs = map(read_config, configs)\n",
    "    return sc.parallelize(configs)\n",
    "\n",
    "def create_configs_df(configs: pyspark.RDD) -> DataFrame:\n",
    "    config_columns = [\n",
    "        \"simulation_id\",\n",
    "        \"slope\",\n",
    "        \"r_stat_multiplier\",\n",
    "        \"effective_radius_multiplier\",\n",
    "        \"min_rim_percentage\"\n",
    "    ]\n",
    "    return configs.map(lambda x: {k: v for k, v in x.items() if k in config_columns}).toDF()\n",
    "\n",
    "def sample_post_saturation_by_simulation(data: DataFrame,\n",
    "                                         configs: pyspark.RDD,\n",
    "                                         n_craters_to_sample: int) -> DataFrame:\n",
    "    configs_df = create_configs_df(configs)\n",
    "\n",
    "    window = Window.partitionBy(\"simulation_id\").orderBy(F.col(\"n_craters_added_in_study_region\"))\n",
    "    craters_with_row_number = data.withColumn(\"row_number\", F.row_number().over(window))\n",
    "    \n",
    "    saturation_points = craters_with_row_number.groupby(\"simulation_id\").agg(F.max(\"row_number\").alias(\"n_craters_max\"))\n",
    "    saturation_points = saturation_points.withColumn(\"saturation_point\", (F.col(\"n_craters_max\") / 3 * 2).cast(\"int\"))\n",
    "    \n",
    "    with_saturation_points = craters_with_row_number.join(saturation_points, on=\"simulation_id\", how=\"inner\")\n",
    "    \n",
    "    filtered = with_saturation_points \\\n",
    "        .filter(F.col(\"row_number\") - F.col(\"saturation_point\") >= 0) \\\n",
    "        .cache() \\\n",
    "        .filter((F.col(\"row_number\") - F.col(\"saturation_point\")) % ((F.col(\"n_craters_max\") - F.col(\"saturation_point\")) / n_craters_to_sample).cast(\"int\") == 0) \\\n",
    "        .drop(\"row_number\") \\\n",
    "        .drop(\"saturation_point\") \\\n",
    "        .drop(\"n_craters_max\")\n",
    "    \n",
    "    return configs_df.join(filtered, on=\"simulation_id\")\n",
    "\n",
    "# def sample_by_simulation(data: DataFrame,\n",
    "#                          configs: pyspark.RDD,\n",
    "#                          n_craters_to_sample: int) -> DataFrame:\n",
    "#     configs_df = create_configs_df(configs)\n",
    "\n",
    "#     window = Window.partitionBy(\"simulation_id\").orderBy(F.col(\"n_craters_added_in_study_region\"))\n",
    "#     craters_with_row_number = data.withColumn(\"row_number\", F.row_number().over(window))\n",
    "    \n",
    "#     saturation_points = craters_with_row_number.groupby(\"simulation_id\").agg(F.max(\"row_number\").alias(\"n_craters_max\"))\n",
    "#     with_saturation_points = craters_with_row_number.join(saturation_points, on=\"simulation_id\", how=\"inner\")\n",
    "    \n",
    "#     filtered = with_saturation_points \\\n",
    "#         .filter(\n",
    "#             ((F.col(\"row_number\") < F.lit(50000)) & (F.col(\"row_number\") % (F.lit(50000) / n_craters_to_sample).cast(\"int\") == 0))\n",
    "#             | ((F.col(\"row_number\") % (F.col(\"n_craters_max\") / n_craters_to_sample)).cast(\"int\") == 0)\n",
    "#         ) \\\n",
    "#         .drop(\"row_number\") \\\n",
    "#         .drop(\"n_craters_max\")\n",
    "    \n",
    "#     return configs_df.join(filtered, on=\"simulation_id\")\n",
    "\n",
    "def sample_by_simulation(data: DataFrame, n_craters_to_sample: int) -> DataFrame:    \n",
    "    w = Window.partitionBy(F.col(\"simulation_id\")).orderBy(F.col(\"rnd_\"))\n",
    "\n",
    "    filtered = (data\n",
    "                   .withColumn(\"rnd_\", F.rand())\n",
    "                   .withColumn(\"rn_\", F.row_number().over(w))\n",
    "                   .where(F.col(\"rn_\") <= n_craters_to_sample)\n",
    "                   .drop(\"rn_\")\n",
    "                   .drop(\"rnd_\")\n",
    "    )\n",
    "    return filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "afd7e7ad-629a-45fa-ad6e-8348d1a651db",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "all_data_df = spark.read.parquet(f\"{base_path}/*/statistics_*.parquet\").repartition(\"simulation_id\")\n",
    "all_data_df = all_data_df.withColumn(\"information_remaining\", F.col(\"n_craters_in_study_region\") / F.col(\"n_craters_added_in_study_region\"))\n",
    "\n",
    "configs_df = create_configs_df(read_configs(base_path))\n",
    "all_data_df = all_data_df.join(F.broadcast(configs_df), on=\"simulation_id\").cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "50ae13f2-05a3-40fd-a4a7-78367140211d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[simulation_id: bigint, crater_id: bigint, n_craters_added_in_study_region: bigint, n_craters_in_study_region: bigint, areal_density: float, areal_density_overlap_2: float, areal_density_overlap_3: float, center_to_center_nearest_neighbor_distance_mean: float, center_to_center_nearest_neighbor_distance_stdev: float, center_to_center_nearest_neighbor_distance_min: float, center_to_center_nearest_neighbor_distance_max: float, rim_to_rim_nearest_neighbor_distance_mean: float, rim_to_rim_nearest_neighbor_distance_stdev: float, rim_to_rim_nearest_neighbor_distance_max: float, n_non_zero_rim_to_rim_nearest_neighbor_distances: bigint, z: float, za: float, information_remaining: double, effective_radius_multiplier: double, min_rim_percentage: double, r_stat_multiplier: double, slope: double]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "simulation_ids = list(configs_df.select(\"simulation_id\").toPandas()[\"simulation_id\"])\n",
    "train_simulation_ids = set(np.random.choice(simulation_ids, replace=False, size=int(len(simulation_ids) * 0.8)))\n",
    "train_df = all_data_df.filter(all_data_df.simulation_id.isin(train_simulation_ids)).cache()\n",
    "test_df = all_data_df.filter(~all_data_df.simulation_id.isin(train_simulation_ids)).cache()\n",
    "all_data_df.unpersist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3570116f-47fd-4ece-b8a3-d35b5ea6bd99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/08/20 17:05:35 WARN MemoryStore: Not enough space to cache rdd_22_174 in memory! (computed 106.0 MiB so far)\n",
      "23/08/20 17:05:35 WARN BlockManager: Persisting block rdd_22_174 to disk instead.\n",
      "23/08/20 17:05:48 WARN MemoryStore: Not enough space to cache rdd_22_191 in memory! (computed 59.7 MiB so far)\n",
      "23/08/20 17:05:48 WARN BlockManager: Persisting block rdd_22_191 to disk instead.\n",
      "23/08/20 17:05:49 WARN MemoryStore: Not enough space to cache rdd_22_190 in memory! (computed 60.1 MiB so far)\n",
      "23/08/20 17:05:49 WARN BlockManager: Persisting block rdd_22_190 to disk instead.\n",
      "23/08/20 17:05:50 WARN MemoryStore: Not enough space to cache rdd_22_189 in memory! (computed 59.6 MiB so far)\n",
      "23/08/20 17:05:50 WARN MemoryStore: Not enough space to cache rdd_22_182 in memory! (computed 104.8 MiB so far)\n",
      "23/08/20 17:05:51 WARN MemoryStore: Not enough space to cache rdd_22_185 in memory! (computed 103.0 MiB so far)\n",
      "23/08/20 17:05:51 WARN MemoryStore: Not enough space to cache rdd_22_186 in memory! (computed 101.4 MiB so far)\n",
      "23/08/20 17:05:51 WARN BlockManager: Persisting block rdd_22_189 to disk instead.\n",
      "23/08/20 17:05:51 WARN BlockManager: Persisting block rdd_22_182 to disk instead.\n",
      "23/08/20 17:05:51 WARN MemoryStore: Not enough space to cache rdd_22_187 in memory! (computed 103.0 MiB so far)\n",
      "23/08/20 17:05:51 WARN MemoryStore: Not enough space to cache rdd_22_193 in memory! (computed 60.7 MiB so far)\n",
      "23/08/20 17:05:51 WARN BlockManager: Persisting block rdd_22_193 to disk instead.\n",
      "23/08/20 17:05:52 WARN BlockManager: Persisting block rdd_22_186 to disk instead.\n",
      "23/08/20 17:05:52 WARN BlockManager: Persisting block rdd_22_185 to disk instead.\n",
      "23/08/20 17:05:52 WARN BlockManager: Persisting block rdd_22_187 to disk instead.\n",
      "23/08/20 17:05:52 WARN MemoryStore: Not enough space to cache rdd_22_171 in memory! (computed 247.7 MiB so far)\n",
      "23/08/20 17:05:52 WARN BlockManager: Persisting block rdd_22_171 to disk instead.\n",
      "23/08/20 17:05:54 WARN MemoryStore: Not enough space to cache rdd_22_180 in memory! (computed 162.5 MiB so far)\n",
      "23/08/20 17:05:54 WARN BlockManager: Persisting block rdd_22_180 to disk instead.\n",
      "23/08/20 17:05:57 WARN MemoryStore: Not enough space to cache rdd_22_194 in memory! (computed 103.3 MiB so far)\n",
      "23/08/20 17:05:57 WARN BlockManager: Persisting block rdd_22_194 to disk instead.\n",
      "23/08/20 17:05:59 WARN MemoryStore: Not enough space to cache rdd_22_180 in memory! (computed 162.5 MiB so far)\n",
      "23/08/20 17:05:59 WARN MemoryStore: Not enough space to cache rdd_22_191 in memory! (computed 104.6 MiB so far)\n",
      "23/08/20 17:06:00 WARN MemoryStore: Not enough space to cache rdd_22_186 in memory! (computed 158.5 MiB so far)\n",
      "23/08/20 17:06:01 WARN MemoryStore: Not enough space to cache rdd_22_187 in memory! (computed 59.1 MiB so far)\n",
      "23/08/20 17:06:01 WARN MemoryStore: Not enough space to cache rdd_22_182 in memory! (computed 15.8 MiB so far)\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/08/20 17:09:36 WARN MemoryStore: Not enough space to cache rdd_22_150 in memory! (computed 162.7 MiB so far)\n",
      "23/08/20 17:09:36 WARN MemoryStore: Not enough space to cache rdd_22_152 in memory! (computed 103.1 MiB so far)\n",
      "23/08/20 17:09:36 WARN MemoryStore: Not enough space to cache rdd_22_154 in memory! (computed 160.1 MiB so far)\n",
      "23/08/20 17:09:37 WARN MemoryStore: Not enough space to cache rdd_22_156 in memory! (computed 102.9 MiB so far)\n",
      "23/08/20 17:09:45 WARN MemoryStore: Not enough space to cache rdd_22_169 in memory! (computed 208.3 MiB so far)\n",
      "23/08/20 17:09:45 WARN MemoryStore: Not enough space to cache rdd_22_171 in memory! (computed 159.3 MiB so far)\n",
      "23/08/20 17:09:47 WARN MemoryStore: Not enough space to cache rdd_22_177 in memory! (computed 161.5 MiB so far)\n",
      "23/08/20 17:09:51 WARN MemoryStore: Not enough space to cache rdd_22_186 in memory! (computed 58.3 MiB so far)\n",
      "23/08/20 17:09:51 WARN MemoryStore: Not enough space to cache rdd_22_185 in memory! (computed 160.8 MiB so far)\n",
      "23/08/20 17:09:53 WARN MemoryStore: Not enough space to cache rdd_22_190 in memory! (computed 253.2 MiB so far)\n",
      "23/08/20 17:09:55 WARN MemoryStore: Not enough space to cache rdd_22_195 in memory! (computed 30.6 MiB so far)\n",
      "23/08/20 17:09:55 WARN MemoryStore: Not enough space to cache rdd_22_193 in memory! (computed 163.6 MiB so far)\n",
      "23/08/20 17:09:56 WARN MemoryStore: Not enough space to cache rdd_22_198 in memory! (computed 58.6 MiB so far)\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/08/20 17:12:17 WARN MemoryStore: Not enough space to cache rdd_22_146 in memory! (computed 58.8 MiB so far)\n",
      "23/08/20 17:12:17 WARN MemoryStore: Not enough space to cache rdd_22_140 in memory! (computed 337.5 MiB so far)\n",
      "23/08/20 17:12:32 WARN MemoryStore: Not enough space to cache rdd_22_170 in memory! (computed 100.9 MiB so far)\n",
      "23/08/20 17:12:41 WARN MemoryStore: Not enough space to cache rdd_22_190 in memory! (computed 104.4 MiB so far)\n",
      "23/08/20 17:12:42 WARN MemoryStore: Not enough space to cache rdd_22_189 in memory! (computed 161.6 MiB so far)\n",
      "23/08/20 17:12:43 WARN MemoryStore: Not enough space to cache rdd_22_195 in memory! (computed 103.6 MiB so far)\n",
      "23/08/20 17:12:45 WARN MemoryStore: Not enough space to cache rdd_22_196 in memory! (computed 100.1 MiB so far)\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "n_craters_to_sample = [\n",
    "    1000,\n",
    "    2500,\n",
    "    5000,\n",
    "]\n",
    "\n",
    "for n in n_craters_to_sample:\n",
    "    print(n)\n",
    "    \n",
    "    sample = sample_by_simulation(train_df, n)\n",
    "    sample.toPandas().to_parquet(f\"{base_path}/train_{n}.parquet\")\n",
    "    \n",
    "    sample = sample_by_simulation(test_df, n)\n",
    "    sample.toPandas().to_parquet(f\"{base_path}/test_{n}.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "eff7b1fa-f2f2-4615-afbd-d017e9237068",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------------+-------------------+------------------+-------------+------------------+\n",
      "|effective_radius_multiplier| min_rim_percentage| r_stat_multiplier|simulation_id|             slope|\n",
      "+---------------------------+-------------------+------------------+-------------+------------------+\n",
      "|         1.5720289373324587| 0.7420289256812818|6.2409269696352165|         1742| 1.670690634676432|\n",
      "|         1.6967911971431182| 0.6917147956304781| 8.746842174402397|         2442|1.0274706363665504|\n",
      "|         1.3417438720222559| 0.6608971907564327| 4.757405687085767|         2127|2.4396714290305175|\n",
      "|          1.565964322050054| 0.4087464541246743| 3.282286438481391|          760|2.7273478754480296|\n",
      "|         1.4016706394635967| 0.6055612704439735| 8.611877609107639|         2241| 2.469363317002159|\n",
      "|         1.7281046486739586|0.38432545785826916| 4.536620648680108|         2232|1.4878623044134602|\n",
      "|          1.657998561315478|0.45368734003822697| 8.005115581059153|          396|2.8042288653121816|\n",
      "|         1.5416613677842135| 0.6693983174177833| 8.906813065172267|          736|2.8680872219421367|\n",
      "|         1.4546941614409694| 0.3541241483694683| 7.793077987278371|           53|2.5978454664300266|\n",
      "|          1.339798444898317| 0.6867707102253076| 7.426451966499343|          655|2.7097834196966746|\n",
      "|         1.3694453432756188| 0.5345915575161544|  6.04408836831158|         1380|  1.70074800179169|\n",
      "|         1.5719713814779834| 0.3469235990449558| 6.020352930262692|          411|1.9209401855725197|\n",
      "|         1.8140574469987518|0.36972047589665746| 3.460766330465159|          484|1.5526733056266477|\n",
      "|         1.1070375185667232|0.32424030089740097| 8.058891124214675|         1200|2.8243216733643024|\n",
      "|         1.2259319611707735|0.26393322310379475| 7.567546947866143|          715| 2.785785476051803|\n",
      "|         1.4788147790810888| 0.6791420811668542| 5.039617134761901|         1588|1.7428566061503004|\n",
      "|          1.797724599931101| 0.7350175160779735| 4.802821148620237|          935|1.2151645861327183|\n",
      "|         1.6303924124066367| 0.4551323879674136| 6.799666091908099|         2437|1.3283858140754687|\n",
      "|          1.322266461446891| 0.4824016705408266|7.6714744947647855|         1461|1.9926369726998552|\n",
      "|         1.3461621090879894| 0.2529656183168248| 5.921924185144646|         1732| 2.071373167920915|\n",
      "+---------------------------+-------------------+------------------+-------------+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "configs_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1b017367-0a42-4526-b341-abb990a62e0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/08/18 16:42:35 WARN CacheManager: Asked to cache already cached data.\n",
      "23/08/19 21:55:40 WARN JavaUtils: Attempt to delete using native Unix OS command failed for path = /tmp/blockmgr-970a4c6c-577a-41c5-89e4-1285f97bc752. Falling back to Java IO way\n",
      "java.io.IOException: Failed to delete: /tmp/blockmgr-970a4c6c-577a-41c5-89e4-1285f97bc752\n",
      "\tat org.apache.spark.network.util.JavaUtils.deleteRecursivelyUsingUnixNative(JavaUtils.java:177)\n",
      "\tat org.apache.spark.network.util.JavaUtils.deleteRecursively(JavaUtils.java:113)\n",
      "\tat org.apache.spark.network.util.JavaUtils.deleteRecursively(JavaUtils.java:94)\n",
      "\tat org.apache.spark.util.Utils$.deleteRecursively(Utils.scala:1231)\n",
      "\tat org.apache.spark.storage.DiskBlockManager.$anonfun$doStop$1(DiskBlockManager.scala:368)\n",
      "\tat org.apache.spark.storage.DiskBlockManager.$anonfun$doStop$1$adapted(DiskBlockManager.scala:364)\n",
      "\tat scala.collection.IndexedSeqOptimized.foreach(IndexedSeqOptimized.scala:36)\n",
      "\tat scala.collection.IndexedSeqOptimized.foreach$(IndexedSeqOptimized.scala:33)\n",
      "\tat scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:198)\n",
      "\tat org.apache.spark.storage.DiskBlockManager.doStop(DiskBlockManager.scala:364)\n",
      "\tat org.apache.spark.storage.DiskBlockManager.stop(DiskBlockManager.scala:359)\n",
      "\tat org.apache.spark.storage.BlockManager.stop(BlockManager.scala:2052)\n",
      "\tat org.apache.spark.SparkEnv.stop(SparkEnv.scala:95)\n",
      "\tat org.apache.spark.SparkContext.$anonfun$stop$25(SparkContext.scala:2175)\n",
      "\tat org.apache.spark.util.Utils$.tryLogNonFatalError(Utils.scala:1509)\n",
      "\tat org.apache.spark.SparkContext.stop(SparkContext.scala:2175)\n",
      "\tat org.apache.spark.SparkContext.stop(SparkContext.scala:2081)\n",
      "\tat org.apache.spark.SparkContext.$anonfun$new$31(SparkContext.scala:664)\n",
      "\tat org.apache.spark.util.SparkShutdownHook.run(ShutdownHookManager.scala:214)\n",
      "\tat org.apache.spark.util.SparkShutdownHookManager.$anonfun$runAll$2(ShutdownHookManager.scala:188)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2088)\n",
      "\tat org.apache.spark.util.SparkShutdownHookManager.$anonfun$runAll$1(ShutdownHookManager.scala:188)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.util.SparkShutdownHookManager.runAll(ShutdownHookManager.scala:188)\n",
      "\tat org.apache.spark.util.SparkShutdownHookManager$$anon$2.run(ShutdownHookManager.scala:178)\n",
      "\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)\n",
      "\tat java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "23/08/19 21:55:40 WARN JavaUtils: Attempt to delete using native Unix OS command failed for path = /tmp/blockmgr-970a4c6c-577a-41c5-89e4-1285f97bc752/28. Falling back to Java IO way\n",
      "java.io.IOException: Failed to delete: /tmp/blockmgr-970a4c6c-577a-41c5-89e4-1285f97bc752/28\n",
      "\tat org.apache.spark.network.util.JavaUtils.deleteRecursivelyUsingUnixNative(JavaUtils.java:177)\n",
      "\tat org.apache.spark.network.util.JavaUtils.deleteRecursively(JavaUtils.java:113)\n",
      "\tat org.apache.spark.network.util.JavaUtils.deleteRecursivelyUsingJavaIO(JavaUtils.java:133)\n",
      "\tat org.apache.spark.network.util.JavaUtils.deleteRecursively(JavaUtils.java:121)\n",
      "\tat org.apache.spark.network.util.JavaUtils.deleteRecursively(JavaUtils.java:94)\n",
      "\tat org.apache.spark.util.Utils$.deleteRecursively(Utils.scala:1231)\n",
      "\tat org.apache.spark.storage.DiskBlockManager.$anonfun$doStop$1(DiskBlockManager.scala:368)\n",
      "\tat org.apache.spark.storage.DiskBlockManager.$anonfun$doStop$1$adapted(DiskBlockManager.scala:364)\n",
      "\tat scala.collection.IndexedSeqOptimized.foreach(IndexedSeqOptimized.scala:36)\n",
      "\tat scala.collection.IndexedSeqOptimized.foreach$(IndexedSeqOptimized.scala:33)\n",
      "\tat scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:198)\n",
      "\tat org.apache.spark.storage.DiskBlockManager.doStop(DiskBlockManager.scala:364)\n",
      "\tat org.apache.spark.storage.DiskBlockManager.stop(DiskBlockManager.scala:359)\n",
      "\tat org.apache.spark.storage.BlockManager.stop(BlockManager.scala:2052)\n",
      "\tat org.apache.spark.SparkEnv.stop(SparkEnv.scala:95)\n",
      "\tat org.apache.spark.SparkContext.$anonfun$stop$25(SparkContext.scala:2175)\n",
      "\tat org.apache.spark.util.Utils$.tryLogNonFatalError(Utils.scala:1509)\n",
      "\tat org.apache.spark.SparkContext.stop(SparkContext.scala:2175)\n",
      "\tat org.apache.spark.SparkContext.stop(SparkContext.scala:2081)\n",
      "\tat org.apache.spark.SparkContext.$anonfun$new$31(SparkContext.scala:664)\n",
      "\tat org.apache.spark.util.SparkShutdownHook.run(ShutdownHookManager.scala:214)\n",
      "\tat org.apache.spark.util.SparkShutdownHookManager.$anonfun$runAll$2(ShutdownHookManager.scala:188)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2088)\n",
      "\tat org.apache.spark.util.SparkShutdownHookManager.$anonfun$runAll$1(ShutdownHookManager.scala:188)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.util.SparkShutdownHookManager.runAll(ShutdownHookManager.scala:188)\n",
      "\tat org.apache.spark.util.SparkShutdownHookManager$$anon$2.run(ShutdownHookManager.scala:178)\n",
      "\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)\n",
      "\tat java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "23/08/19 21:55:41 WARN JavaUtils: Attempt to delete using native Unix OS command failed for path = /tmp/blockmgr-970a4c6c-577a-41c5-89e4-1285f97bc752/2f. Falling back to Java IO way\n",
      "java.io.IOException: Failed to delete: /tmp/blockmgr-970a4c6c-577a-41c5-89e4-1285f97bc752/2f\n",
      "\tat org.apache.spark.network.util.JavaUtils.deleteRecursivelyUsingUnixNative(JavaUtils.java:177)\n",
      "\tat org.apache.spark.network.util.JavaUtils.deleteRecursively(JavaUtils.java:113)\n",
      "\tat org.apache.spark.network.util.JavaUtils.deleteRecursivelyUsingJavaIO(JavaUtils.java:133)\n",
      "\tat org.apache.spark.network.util.JavaUtils.deleteRecursively(JavaUtils.java:121)\n",
      "\tat org.apache.spark.network.util.JavaUtils.deleteRecursively(JavaUtils.java:94)\n",
      "\tat org.apache.spark.util.Utils$.deleteRecursively(Utils.scala:1231)\n",
      "\tat org.apache.spark.storage.DiskBlockManager.$anonfun$doStop$1(DiskBlockManager.scala:368)\n",
      "\tat org.apache.spark.storage.DiskBlockManager.$anonfun$doStop$1$adapted(DiskBlockManager.scala:364)\n",
      "\tat scala.collection.IndexedSeqOptimized.foreach(IndexedSeqOptimized.scala:36)\n",
      "\tat scala.collection.IndexedSeqOptimized.foreach$(IndexedSeqOptimized.scala:33)\n",
      "\tat scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:198)\n",
      "\tat org.apache.spark.storage.DiskBlockManager.doStop(DiskBlockManager.scala:364)\n",
      "\tat org.apache.spark.storage.DiskBlockManager.stop(DiskBlockManager.scala:359)\n",
      "\tat org.apache.spark.storage.BlockManager.stop(BlockManager.scala:2052)\n",
      "\tat org.apache.spark.SparkEnv.stop(SparkEnv.scala:95)\n",
      "\tat org.apache.spark.SparkContext.$anonfun$stop$25(SparkContext.scala:2175)\n",
      "\tat org.apache.spark.util.Utils$.tryLogNonFatalError(Utils.scala:1509)\n",
      "\tat org.apache.spark.SparkContext.stop(SparkContext.scala:2175)\n",
      "\tat org.apache.spark.SparkContext.stop(SparkContext.scala:2081)\n",
      "\tat org.apache.spark.SparkContext.$anonfun$new$31(SparkContext.scala:664)\n",
      "\tat org.apache.spark.util.SparkShutdownHook.run(ShutdownHookManager.scala:214)\n",
      "\tat org.apache.spark.util.SparkShutdownHookManager.$anonfun$runAll$2(ShutdownHookManager.scala:188)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2088)\n",
      "\tat org.apache.spark.util.SparkShutdownHookManager.$anonfun$runAll$1(ShutdownHookManager.scala:188)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.util.SparkShutdownHookManager.runAll(ShutdownHookManager.scala:188)\n",
      "\tat org.apache.spark.util.SparkShutdownHookManager$$anon$2.run(ShutdownHookManager.scala:178)\n",
      "\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)\n",
      "\tat java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n"
     ]
    }
   ],
   "source": [
    "configs_df_repartitioned = configs_df.repartition(\"simulation_id\").cache()\n",
    "\n",
    "for simulation_id in simulation_ids:\n",
    "    filtered = all_data_df.filter(F.col(\"simulation_id\") == simulation_id).sort(F.col(\"n_craters_added_in_study_region\"))\n",
    "    filtered.toPandas().to_parquet(f\"{base_path}/simulation_{simulation_id}.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d149e8b6-3cc5-4791-b3de-ac1abe57a6bf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
