{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34fb5b29-5fdb-4d6c-bc8c-9728b34e51c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import glob\n",
    "import yaml\n",
    "from typing import *\n",
    "from functools import reduce\n",
    "\n",
    "import pyspark\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql import SparkSession, DataFrame, Row, Window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b36eff61-ec11-42d5-ada4-3c9c7c3586fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_path = \"/data/saturation/central_composite_design/ccd6\"\n",
    "base_path_holdout = \"/data/saturation/central_composite_design/ccd6_holdout_larger\"\n",
    "\n",
    "holdout_slopes = {1, 2, 3}\n",
    "holdout_effective_radius_multipliers = {1.1, 1.5, 1.9}\n",
    "holdout_min_rim_percentages = {.25, .475, .7}\n",
    "holdout_r_stat_multipliers = {3, 6, 9}\n",
    "\n",
    "# base_path = \"/data/saturation/central_composite_design/ccd9\"\n",
    "# base_path_holdout = \"/data/saturation/central_composite_design/ccd9_holdout\"\n",
    "\n",
    "# holdout_slopes = {1, 1.5, 2, 2.5, 3}\n",
    "# holdout_effective_radius_multipliers = {1.1, 1.3, 1.5, 1.7, 1.9}\n",
    "# holdout_min_rim_percentages = {.25, .375, .5, .625, .75}\n",
    "# holdout_r_stat_multipliers = {3, 4.5, 6, 7.5, 9}\n",
    "\n",
    "n_cores = 28"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "489393ae-5872-4a8b-a8ac-551df0a6d30d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_config(path: Path) -> Dict:\n",
    "    with path.open(\"r\") as config_file:\n",
    "        config = yaml.safe_load(config_file)\n",
    "    return config\n",
    "\n",
    "\n",
    "def get_aggregations_for_column(col: str) -> Iterable:\n",
    "    # Percentiles 5 to 95 step 5\n",
    "    for quantile in range(1, 20):\n",
    "        yield F.percentile_approx(col, quantile / 20, accuracy=int(1e6)).alias(f\"{col}_{quantile*5:.0f}_percentile\")\n",
    "    \n",
    "    yield F.percentile_approx(col, .99, accuracy=int(1e6)).alias(f\"{col}_99_percentile\")\n",
    "    yield F.percentile_approx(col, .50, accuracy=int(1e6)).alias(f\"{col}_median\")\n",
    "    yield F.min(col).alias(f\"{col}_min\")\n",
    "    yield F.max(col).alias(f\"{col}_max\")\n",
    "    yield F.mean(col).alias(f\"{col}_mean\")\n",
    "    yield F.stddev_samp(col).alias(f\"{col}_stdev\")   \n",
    "    \n",
    "\n",
    "def calculate_stats(data: DataFrame) -> DataFrame:\n",
    "    # Number of craters from the end of the simulation to consider as in saturation\n",
    "    N_CRATERS_IN_SATURATION = 50000\n",
    "    \n",
    "    columns_to_calculate_stats = [\n",
    "        \"areal_density\",\n",
    "        \"z\",\n",
    "        \"za\",\n",
    "        \"n_craters_in_study_region\",\n",
    "        \"n_craters_added_in_study_region\"\n",
    "    ]\n",
    "    \n",
    "    # Grab the last N_CRATERS_IN_SATURATION craters from each simulation\n",
    "    window = Window.partitionBy(\"simulation_id\").orderBy(F.col(\"n_craters_added_in_study_region\").desc())\n",
    "    last_n_craters_by_sim = data.withColumn(\"row_number\", F.row_number().over(window)) \\\n",
    "        .filter(F.col(\"row_number\") <= 50) \\\n",
    "        .drop(\"row_number\")\n",
    "\n",
    "    # Set up the aggregations for each column of interest\n",
    "    aggregations = [\n",
    "        aggregation\n",
    "        for col in columns_to_calculate_stats\n",
    "        for aggregation in get_aggregations_for_column(col)\n",
    "    ]\n",
    "\n",
    "    return last_n_craters_by_sim.groupBy(\"simulation_id\").agg(*aggregations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "628fdb7b-c860-4871-98c6-aefdac2cb9fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder \\\n",
    "                    .master(f\"local[{n_cores}]\") \\\n",
    "                    .appName(\"Saturation\") \\\n",
    "                    .config(\"spark.driver.memory\", \"48g\") \\\n",
    "                    .getOrCreate()\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34c89064-9f9d-4b58-bfd4-e74d1451d6fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read statistics\n",
    "stats_df = spark.read.parquet(f\"{base_path}/*/statistics_*.parquet\").cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a94471b-e77b-416d-a819-2e7a4898e75e",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = calculate_stats(stats_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ef05905-c1b7-4ae2-bda9-360e161d5392",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read configs\n",
    "completed_filenames = list(Path(base_path).glob(\"*/completed.txt\"))\n",
    "configs = map(lambda x: x.parent / \"config.yaml\", completed_filenames)\n",
    "configs = map(read_config, configs)\n",
    "configs = sc.parallelize(configs)\n",
    "\n",
    "config_columns = [\n",
    "    \"simulation_id\",\n",
    "    \"slope\",\n",
    "    \"r_stat_multiplier\",\n",
    "    \"effective_radius_multiplier\",\n",
    "    \"min_rim_percentage\"\n",
    "]\n",
    "configs_df = configs.toDF().select(config_columns).repartition(\"simulation_id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ea7d26a-eae5-46e9-a2e5-cb8213df0bb8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ed6b11e-40cd-4017-bfb0-e1675bd56f58",
   "metadata": {},
   "outputs": [],
   "source": [
    "joined = configs_df.join(data, on=\"simulation_id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0a47855-0336-4b1b-acbb-26754ac5fced",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write out the result\n",
    "joined.toPandas().to_csv(f\"{base_path}/post_saturation_statistics.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f125784-7250-461c-b364-c42798271e11",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afb4a61e-9af1-40fc-8aed-255bac0c73cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare samples from each simulation, overall and post-saturation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b48108ca-f791-477f-9492-8c85363839c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_config(path: Path) -> Dict:\n",
    "    with path.open(\"r\") as config_file:\n",
    "        config = yaml.safe_load(config_file)\n",
    "    return config\n",
    "\n",
    "def read_configs(base_path: str) -> pyspark.RDD:\n",
    "    completed_filenames = list(Path(base_path).glob(\"*/completed.txt\"))\n",
    "    configs = map(lambda x: x.parent / \"config.yaml\", completed_filenames)\n",
    "    configs = map(read_config, configs)\n",
    "    return sc.parallelize(configs)\n",
    "\n",
    "def create_configs_df(configs: pyspark.RDD) -> DataFrame:\n",
    "    config_columns = [\n",
    "        \"simulation_id\",\n",
    "        \"slope\",\n",
    "        \"r_stat_multiplier\",\n",
    "        \"effective_radius_multiplier\",\n",
    "        \"min_rim_percentage\"\n",
    "    ]\n",
    "    return configs.toDF().select(config_columns)\n",
    "\n",
    "def sample_post_saturation_by_simulation(data: DataFrame,\n",
    "                                         configs: pyspark.RDD,\n",
    "                                         n_craters_to_sample: int) -> DataFrame:\n",
    "    configs_df = create_configs_df(configs)\n",
    "\n",
    "    window = Window.partitionBy(\"simulation_id\").orderBy(F.col(\"n_craters_added_in_study_region\"))\n",
    "    craters_with_row_number = data.withColumn(\"row_number\", F.row_number().over(window))\n",
    "    \n",
    "    saturation_points = craters_with_row_number.groupby(\"simulation_id\").agg(F.max(\"row_number\").alias(\"n_craters_max\"))\n",
    "    saturation_points = saturation_points.withColumn(\"saturation_point\", (F.col(\"n_craters_max\") / 3 * 2).cast(\"int\"))\n",
    "    \n",
    "    with_saturation_points = craters_with_row_number.join(saturation_points, on=\"simulation_id\", how=\"inner\")\n",
    "    \n",
    "    filtered = with_saturation_points \\\n",
    "        .filter(F.col(\"row_number\") - F.col(\"saturation_point\") >= 0) \\\n",
    "        .cache() \\\n",
    "        .filter((F.col(\"row_number\") - F.col(\"saturation_point\")) % ((F.col(\"n_craters_max\") - F.col(\"saturation_point\")) / n_craters_to_sample).cast(\"int\") == 0) \\\n",
    "        .drop(\"row_number\") \\\n",
    "        .drop(\"saturation_point\") \\\n",
    "        .drop(\"n_craters_max\")\n",
    "    \n",
    "    return configs_df.join(filtered, on=\"simulation_id\")\n",
    "\n",
    "def sample_by_simulation(data: DataFrame,\n",
    "                         configs: pyspark.RDD,\n",
    "                         n_craters_to_sample: int) -> DataFrame:\n",
    "    configs_df = create_configs_df(configs)\n",
    "\n",
    "    window = Window.partitionBy(\"simulation_id\").orderBy(F.col(\"n_craters_added_in_study_region\"))\n",
    "    craters_with_row_number = data.withColumn(\"row_number\", F.row_number().over(window))\n",
    "    \n",
    "    saturation_points = craters_with_row_number.groupby(\"simulation_id\").agg(F.max(\"row_number\").alias(\"n_craters_max\"))\n",
    "    with_saturation_points = craters_with_row_number.join(saturation_points, on=\"simulation_id\", how=\"inner\")\n",
    "    \n",
    "    filtered = with_saturation_points \\\n",
    "        .filter(\n",
    "            ((F.col(\"row_number\") < F.lit(50000)) & (F.col(\"row_number\") % (F.lit(50000) / n_craters_to_sample).cast(\"int\") == 0))\n",
    "            | ((F.col(\"row_number\") % (F.col(\"n_craters_max\") / n_craters_to_sample)).cast(\"int\") == 0)\n",
    "        ) \\\n",
    "        .drop(\"row_number\") \\\n",
    "        .drop(\"n_craters_max\")\n",
    "    \n",
    "    return configs_df.join(filtered, on=\"simulation_id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df282b0e-a6cb-4f8b-8443-9469b5261c69",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_configs = read_configs(base_path)\n",
    "configs = train_configs.toDF().toPandas()\n",
    "\n",
    "in_sample_holdout_ids = configs.simulation_id[configs.slope.isin(holdout_slopes)\n",
    "                                              & configs.effective_radius_multiplier.isin(holdout_effective_radius_multipliers)\n",
    "                                              & configs.min_rim_percentage.isin(holdout_min_rim_percentages)\n",
    "                                              & configs.r_stat_multiplier.isin(holdout_r_stat_multipliers)]\n",
    "in_sample_holdout_ids = set(in_sample_holdout_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fe6e233-d7ae-4d3a-9218-57dd15e88fb7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5ed5541-19c2-4b81-9390-740538c343f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = spark.read.parquet(f\"{base_path}/*/statistics_*.parquet\").repartition(\"simulation_id\").cache()\n",
    "\n",
    "train_configs = read_configs(base_path).cache()\n",
    "\n",
    "in_sample_holdout_df = train_df.filter(train_df.simulation_id.isin(in_sample_holdout_ids))\n",
    "train_df = train_df.filter(~train_df.simulation_id.isin(in_sample_holdout_ids))\n",
    "\n",
    "holdout_df = spark.read.parquet(f\"{base_path_holdout}/*/statistics_*.parquet\").cache()\n",
    "holdout_configs = read_configs(base_path_holdout)\n",
    "\n",
    "n_craters_to_sample = [\n",
    "    100,\n",
    "    500,\n",
    "    1000,\n",
    "    5000,\n",
    "    10000\n",
    "]\n",
    "for n in n_craters_to_sample:\n",
    "    sample = sample_post_saturation_by_simulation(train_df, train_configs, n)\n",
    "    sample.toPandas().to_csv(f\"{base_path}/post_saturation_sample_{n}.csv\", index=False)\n",
    "    \n",
    "    sample = sample_post_saturation_by_simulation(in_sample_holdout_df, train_configs, n)\n",
    "    sample.toPandas().to_csv(f\"{base_path}/post_saturation_in_sample_holdout_sample_{n}.csv\", index=False)\n",
    "    \n",
    "    sample = sample_post_saturation_by_simulation(holdout_df, holdout_configs, n)\n",
    "    sample.toPandas().to_csv(f\"{base_path_holdout}/post_saturation_sample_{n}.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "233c843f-c814-428c-b830-eaea408b4c3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = spark.read.parquet(f\"{base_path}/*/statistics_*.parquet\").repartition(\"simulation_id\").cache()\n",
    "\n",
    "train_configs = read_configs(base_path).cache()\n",
    "\n",
    "in_sample_holdout_df = train_df.filter(train_df.simulation_id.isin(in_sample_holdout_ids))\n",
    "train_df = train_df.filter(~train_df.simulation_id.isin(in_sample_holdout_ids))\n",
    "\n",
    "holdout_df = spark.read.parquet(f\"{base_path_holdout}/*/statistics_*.parquet\").cache()\n",
    "holdout_configs = read_configs(base_path_holdout)\n",
    "\n",
    "n_craters_to_sample = [\n",
    "    100,\n",
    "    500,\n",
    "    1000,\n",
    "    5000,\n",
    "    10000\n",
    "]\n",
    "for n in n_craters_to_sample:\n",
    "    sample = sample_by_simulation(train_df, train_configs, n)\n",
    "    sample.toPandas().to_csv(f\"{base_path}/sample_{n}.csv\", index=False)\n",
    "    \n",
    "    sample = sample_by_simulation(in_sample_holdout_df, train_configs, n)\n",
    "    sample.toPandas().to_csv(f\"{base_path}/in_sample_holdout_sample_{n}.csv\", index=False)\n",
    "    \n",
    "    sample = sample_by_simulation(holdout_df, holdout_configs, n)\n",
    "    sample.toPandas().to_csv(f\"{base_path_holdout}/sample_{n}.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30ca4ae0-98c0-48da-82af-e9ff8b0a71bf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "056ff94e-34f2-487e-a36e-af344eedc472",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_configs_df = train_configs.toDF().repartition(\"simulation_id\").cache()\n",
    "\n",
    "for holdout_simulation_id in in_sample_holdout_ids:\n",
    "    filtered = in_sample_holdout_df.filter(F.col(\"simulation_id\") == holdout_simulation_id)\n",
    "    joined = train_configs_df.join(filtered, on=\"simulation_id\").sort(F.col(\"n_craters_added_in_study_region\"))\n",
    "    joined.toPandas().to_parquet(f\"{base_path}/simulation_{holdout_simulation_id}.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44c751bc-3bed-4059-a052-8cb4ebb5f94d",
   "metadata": {},
   "outputs": [],
   "source": [
    "holdout_configs_df = holdout_configs.toDF().repartition(\"simulation_id\").cache()\n",
    "holdout_simulation_ids = [x[\"simulation_id\"] for x in holdout_configs.collect()]\n",
    "\n",
    "for holdout_simulation_id in holdout_simulation_ids:\n",
    "    filtered = holdout_df.filter(F.col(\"simulation_id\") == holdout_simulation_id)\n",
    "    joined = holdout_configs_df.join(filtered, on=\"simulation_id\").sort(F.col(\"n_craters_added_in_study_region\"))\n",
    "    joined.toPandas().to_parquet(f\"{base_path_holdout}/simulation_{holdout_simulation_id}.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f8983a3-ed15-46bb-96cd-a147a7056ed0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
