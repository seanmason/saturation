{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "34fb5b29-5fdb-4d6c-bc8c-9728b34e51c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import glob\n",
    "import yaml\n",
    "from typing import *\n",
    "from functools import reduce\n",
    "\n",
    "import pyspark\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql import SparkSession, DataFrame, Row, Window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b36eff61-ec11-42d5-ada4-3c9c7c3586fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_path = \"/data/saturation/n_craters_stop_condition_20230918/\"\n",
    "\n",
    "n_cores = 28"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "628fdb7b-c860-4871-98c6-aefdac2cb9fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/09/23 12:09:44 WARN Utils: Your hostname, muninn resolves to a loopback address: 127.0.1.1; using 192.168.86.20 instead (on interface enp8s0)\n",
      "23/09/23 12:09:44 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "23/09/23 12:09:44 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "spark = (SparkSession.builder\n",
    "                     .master(f\"local[{n_cores}]\")\n",
    "                     .appName(\"Saturation\")\n",
    "                     .config(\"spark.sql.shuffle.partitions\", \"1000\")\n",
    "                     .config(\"spark.driver.memory\", \"64g\")\n",
    "                     .getOrCreate())\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b48108ca-f791-477f-9492-8c85363839c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_config(path: Path) -> Dict:\n",
    "    with path.open(\"r\") as config_file:\n",
    "        config = yaml.safe_load(config_file)\n",
    "    return config\n",
    "\n",
    "def read_configs(base_path: str) -> pyspark.RDD:\n",
    "    completed_filenames = list(Path(base_path).glob(\"*/completed.txt\"))\n",
    "    configs = map(lambda x: x.parent / \"config.yaml\", completed_filenames)\n",
    "    configs = map(read_config, configs)\n",
    "    return sc.parallelize(configs)\n",
    "\n",
    "def create_configs_df(configs: pyspark.RDD) -> DataFrame:\n",
    "    config_columns = [\n",
    "        \"simulation_id\",\n",
    "        \"slope\",\n",
    "        \"r_stat_multiplier\",\n",
    "        \"effective_radius_multiplier\",\n",
    "        \"min_rim_percentage\"\n",
    "    ]\n",
    "    return configs.map(lambda x: {k: v for k, v in x.items() if k in config_columns}).toDF()\n",
    "\n",
    "def sample_by_simulation(data: DataFrame,\n",
    "                         n_samples_per_simulation: int) -> DataFrame:\n",
    "    \"\"\"\n",
    "    Samples n_samples_per_simulation samples from each simulation.\n",
    "    \"\"\"\n",
    "    window = Window.partitionBy(\"simulation_id\").orderBy(\"rnd_\")\n",
    "\n",
    "    filtered = (data\n",
    "                   .withColumn(\"rnd_\", F.rand())\n",
    "                   .withColumn(\"rn_\", F.row_number().over(window))\n",
    "                   .where(F.col(\"rn_\") <= n_samples_per_simulation)\n",
    "                   .drop(\"rn_\")\n",
    "                   .drop(\"rnd_\")\n",
    "    )\n",
    "    \n",
    "    return filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a3f3fda1-ebd6-48f9-9c4f-9c4ed194194c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_post_saturation_percentiles(data: DataFrame, column: str):\n",
    "    \"\"\"\n",
    "    Calculates the post-saturation percentile of a given column.\n",
    "    \"\"\"\n",
    "    col_dtype = dict(data.dtypes)[column]\n",
    "    \n",
    "    # Select all points post-saturation - last 1/3 of each simulation\n",
    "    window = Window.partitionBy(\"simulation_id\").orderBy(F.col(\"n_craters_added_in_study_region\"))\n",
    "    with_row_number = data.withColumn(\"row_number\", F.row_number().over(window))\n",
    "    \n",
    "    saturation_points = with_row_number.groupby(\"simulation_id\").agg(F.max(\"row_number\").alias(\"n_rows\"))\n",
    "    saturation_points = saturation_points.withColumn(\"saturation_point\", (F.col(\"n_rows\") / 3 * 2).cast(\"int\"))\n",
    "    \n",
    "    with_saturation_points = with_row_number.join(saturation_points, on=\"simulation_id\", how=\"inner\")\n",
    "    post_saturation = (\n",
    "        with_saturation_points\n",
    "        .filter(F.col(\"row_number\") - F.col(\"saturation_point\") >= 0)\n",
    "        .drop(\"row_number\")\n",
    "        .drop(\"saturation_point\")\n",
    "        .drop(\"n_rows\")\n",
    "    )\n",
    "    \n",
    "    # Calculate post-saturation percentiles for each simulation\n",
    "    # Create a \"lookup table\" of percentiles by simulation to join to\n",
    "    percentile_lookup = (\n",
    "        post_saturation\n",
    "        .groupby(\"simulation_id\")\n",
    "        .agg(\n",
    "            F.percentile_approx(column, F.array(*[F.lit(x / 100.0) for x in range(1, 100)]), 10000).alias(\"percentiles\")\n",
    "        )\n",
    "        .select(\n",
    "            \"simulation_id\",\n",
    "            F.explode(\n",
    "                F.arrays_zip(\n",
    "                    F.array(*[F.lit(x / 100) for x in range(0, 100)]),\n",
    "                    F.array_insert(\"percentiles\", 1, F.lit(-2**33).cast(col_dtype)),\n",
    "                    F.array_insert(\"percentiles\", 100, F.lit(2**33).cast(col_dtype)),\n",
    "                )\n",
    "            ).alias(\"percentile_array\")       \n",
    "        )\n",
    "        .select(\n",
    "            \"simulation_id\",\n",
    "            F.col(\"percentile_array\")[\"0\"].alias(f\"post_saturation_{column}_percentile\"),\n",
    "            F.col(\"percentile_array\")[\"1\"].alias(\"lower\"),\n",
    "            F.col(\"percentile_array\")[\"2\"].alias(\"upper\"),\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # Join back to the full dataframe to add percentiles to each observation\n",
    "    result = (\n",
    "        data\n",
    "        .join(percentile_lookup, on=\"simulation_id\")\n",
    "        .filter(post_saturation[column] >= percentile_lookup.lower)\n",
    "        .filter(post_saturation[column] < percentile_lookup.upper)\n",
    "        .drop(\"lower\", \"upper\")\n",
    "    )\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8f224012-a295-403b-9efa-b63b6898c1d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "configs_df = create_configs_df(read_configs(base_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d2a22187-36b4-4bc1-b72a-da12223ae082",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select at most n_simulations from all available simulations\n",
    "n_simulations = 500\n",
    "simulation_ids = list(configs_df.select(\"simulation_id\").toPandas()[\"simulation_id\"])\n",
    "n_simulations = min(n_simulations, len(simulation_ids))\n",
    "simulation_ids = set(np.random.choice(simulation_ids, replace=False, size=n_simulations))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d06c04cf-8a63-4e17-b43d-862880b9a329",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "data = spark.read.parquet(f\"{base_path}/*/statistics_*.parquet\")\n",
    "data = data.filter(data.simulation_id.isin(simulation_ids))\n",
    "data = data.withColumn(\"information_remaining\", F.col(\"n_craters_in_study_region\") / F.col(\"n_craters_added_in_study_region\")).cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "263a80a0-6c9d-4fe6-bafb-afd0c12ae4d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = add_post_saturation_percentiles(data, \"n_craters_in_study_region\")\n",
    "data = add_post_saturation_percentiles(data, \"areal_density\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34f93ed4-2e1e-4701-b737-a80063c10f14",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "747bc07a-9ccc-45c8-9e0f-dbe79d81a9d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.join(F.broadcast(configs_df), on=\"simulation_id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "50ae13f2-05a3-40fd-a4a7-78367140211d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split into train and test sets\n",
    "train_simulation_ids = set(np.random.choice(list(simulation_ids), replace=False, size=int(n_simulations * 0.8)))\n",
    "test_simulation_ids = set([x for x in simulation_ids if x not in train_simulation_ids])\n",
    "train_df = data.filter(data.simulation_id.isin(train_simulation_ids))\n",
    "test_df = data.filter(data.simulation_id.isin(test_simulation_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3570116f-47fd-4ece-b8a3-d35b5ea6bd99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                ]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "250\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                ]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/09/23 12:23:16 WARN JavaUtils: Attempt to delete using native Unix OS command failed for path = /tmp/blockmgr-0a458a4a-6515-4409-bbdc-536b59b07c74. Falling back to Java IO way\n",
      "java.io.IOException: Failed to delete: /tmp/blockmgr-0a458a4a-6515-4409-bbdc-536b59b07c74\n",
      "\tat org.apache.spark.network.util.JavaUtils.deleteRecursivelyUsingUnixNative(JavaUtils.java:177)\n",
      "\tat org.apache.spark.network.util.JavaUtils.deleteRecursively(JavaUtils.java:113)\n",
      "\tat org.apache.spark.network.util.JavaUtils.deleteRecursively(JavaUtils.java:94)\n",
      "\tat org.apache.spark.util.Utils$.deleteRecursively(Utils.scala:1231)\n",
      "\tat org.apache.spark.storage.DiskBlockManager.$anonfun$doStop$1(DiskBlockManager.scala:368)\n",
      "\tat org.apache.spark.storage.DiskBlockManager.$anonfun$doStop$1$adapted(DiskBlockManager.scala:364)\n",
      "\tat scala.collection.IndexedSeqOptimized.foreach(IndexedSeqOptimized.scala:36)\n",
      "\tat scala.collection.IndexedSeqOptimized.foreach$(IndexedSeqOptimized.scala:33)\n",
      "\tat scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:198)\n",
      "\tat org.apache.spark.storage.DiskBlockManager.doStop(DiskBlockManager.scala:364)\n",
      "\tat org.apache.spark.storage.DiskBlockManager.stop(DiskBlockManager.scala:359)\n",
      "\tat org.apache.spark.storage.BlockManager.stop(BlockManager.scala:2052)\n",
      "\tat org.apache.spark.SparkEnv.stop(SparkEnv.scala:95)\n",
      "\tat org.apache.spark.SparkContext.$anonfun$stop$25(SparkContext.scala:2175)\n",
      "\tat org.apache.spark.util.Utils$.tryLogNonFatalError(Utils.scala:1509)\n",
      "\tat org.apache.spark.SparkContext.stop(SparkContext.scala:2175)\n",
      "\tat org.apache.spark.SparkContext.stop(SparkContext.scala:2081)\n",
      "\tat org.apache.spark.SparkContext.$anonfun$new$31(SparkContext.scala:664)\n",
      "\tat org.apache.spark.util.SparkShutdownHook.run(ShutdownHookManager.scala:214)\n",
      "\tat org.apache.spark.util.SparkShutdownHookManager.$anonfun$runAll$2(ShutdownHookManager.scala:188)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2088)\n",
      "\tat org.apache.spark.util.SparkShutdownHookManager.$anonfun$runAll$1(ShutdownHookManager.scala:188)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.util.SparkShutdownHookManager.runAll(ShutdownHookManager.scala:188)\n",
      "\tat org.apache.spark.util.SparkShutdownHookManager$$anon$2.run(ShutdownHookManager.scala:178)\n",
      "\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)\n",
      "\tat java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "23/09/23 12:23:16 WARN JavaUtils: Attempt to delete using native Unix OS command failed for path = /tmp/blockmgr-0a458a4a-6515-4409-bbdc-536b59b07c74/24. Falling back to Java IO way\n",
      "java.io.IOException: Failed to delete: /tmp/blockmgr-0a458a4a-6515-4409-bbdc-536b59b07c74/24\n",
      "\tat org.apache.spark.network.util.JavaUtils.deleteRecursivelyUsingUnixNative(JavaUtils.java:177)\n",
      "\tat org.apache.spark.network.util.JavaUtils.deleteRecursively(JavaUtils.java:113)\n",
      "\tat org.apache.spark.network.util.JavaUtils.deleteRecursivelyUsingJavaIO(JavaUtils.java:133)\n",
      "\tat org.apache.spark.network.util.JavaUtils.deleteRecursively(JavaUtils.java:121)\n",
      "\tat org.apache.spark.network.util.JavaUtils.deleteRecursively(JavaUtils.java:94)\n",
      "\tat org.apache.spark.util.Utils$.deleteRecursively(Utils.scala:1231)\n",
      "\tat org.apache.spark.storage.DiskBlockManager.$anonfun$doStop$1(DiskBlockManager.scala:368)\n",
      "\tat org.apache.spark.storage.DiskBlockManager.$anonfun$doStop$1$adapted(DiskBlockManager.scala:364)\n",
      "\tat scala.collection.IndexedSeqOptimized.foreach(IndexedSeqOptimized.scala:36)\n",
      "\tat scala.collection.IndexedSeqOptimized.foreach$(IndexedSeqOptimized.scala:33)\n",
      "\tat scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:198)\n",
      "\tat org.apache.spark.storage.DiskBlockManager.doStop(DiskBlockManager.scala:364)\n",
      "\tat org.apache.spark.storage.DiskBlockManager.stop(DiskBlockManager.scala:359)\n",
      "\tat org.apache.spark.storage.BlockManager.stop(BlockManager.scala:2052)\n",
      "\tat org.apache.spark.SparkEnv.stop(SparkEnv.scala:95)\n",
      "\tat org.apache.spark.SparkContext.$anonfun$stop$25(SparkContext.scala:2175)\n",
      "\tat org.apache.spark.util.Utils$.tryLogNonFatalError(Utils.scala:1509)\n",
      "\tat org.apache.spark.SparkContext.stop(SparkContext.scala:2175)\n",
      "\tat org.apache.spark.SparkContext.stop(SparkContext.scala:2081)\n",
      "\tat org.apache.spark.SparkContext.$anonfun$new$31(SparkContext.scala:664)\n",
      "\tat org.apache.spark.util.SparkShutdownHook.run(ShutdownHookManager.scala:214)\n",
      "\tat org.apache.spark.util.SparkShutdownHookManager.$anonfun$runAll$2(ShutdownHookManager.scala:188)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2088)\n",
      "\tat org.apache.spark.util.SparkShutdownHookManager.$anonfun$runAll$1(ShutdownHookManager.scala:188)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.util.SparkShutdownHookManager.runAll(ShutdownHookManager.scala:188)\n",
      "\tat org.apache.spark.util.SparkShutdownHookManager$$anon$2.run(ShutdownHookManager.scala:178)\n",
      "\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)\n",
      "\tat java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n"
     ]
    }
   ],
   "source": [
    "n_samples_per_simulation = [\n",
    "    100,\n",
    "    250,\n",
    "    500,\n",
    "]\n",
    "\n",
    "for n in n_samples_per_simulation:\n",
    "    print(n)\n",
    "    \n",
    "    sample = sample_by_simulation(train_df, n)\n",
    "    sample.toPandas().to_parquet(f\"{base_path}/train_{n_simulations}_{n}.parquet\")\n",
    "    \n",
    "    sample = sample_by_simulation(test_df, n)\n",
    "    sample.toPandas().to_parquet(f\"{base_path}/test_{n_simulations}_{n}.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "135b263a-812f-41c8-9ead-7b24d6f2e859",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
